{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d218aa69-63bf-45bc-aa97-53ac1bf0168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "# # Plotting utils \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker \n",
    "import matplotlib.patches as patches\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# Analysis\n",
    "import time\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "import datetime\n",
    "from   datetime import date, timedelta\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "import glob\n",
    "import os \n",
    "\n",
    "import metpy.calc as mpc\n",
    "from metpy.units import units\n",
    "from metpy.plots import Hodograph, SkewT\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Import Ngl with pyn_env active \n",
    "import Ngl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5782be-f596-4d6d-ab55-6369ad8acc2b",
   "metadata": {},
   "source": [
    "**Useful functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2576f112-953e-45fe-98db-c8724990c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from Brian M. to use time midpoints, not end periods\n",
    "def cesm_correct_time(ds):\n",
    "    \"\"\"Given a Dataset, check for time_bnds,\n",
    "       and use avg(time_bnds) to replace the time coordinate.\n",
    "       Purpose is to center the timestamp on the averaging inverval.   \n",
    "       NOTE: ds should have been loaded using `decode_times=False`\n",
    "    \"\"\"\n",
    "    assert 'time_bnds' in ds\n",
    "    assert 'time' in ds\n",
    "    correct_time_values = ds['time_bnds'].mean(dim='nbnd')\n",
    "    # copy any metadata:\n",
    "    correct_time_values.attrs = ds['time'].attrs\n",
    "    ds = ds.assign_coords({\"time\": correct_time_values})\n",
    "    ds = xr.decode_cf(ds)  # decode to datetime objects\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9eb0d7-2ae8-417d-962c-74b424ae296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function to get the height of the PBL as the level with maximum d(var)/dz. \n",
    "Inputs:  A dataset with CAM output ('DS') and the variable to differentiate ('pbl_var')\n",
    "Outputs: An array with boundary layer depth\n",
    "'''\n",
    "def PBLHasMaxDZ(DS, pbl_var): \n",
    "    # Convert HMGcamDS_all to height (nabbed from Rich's script)\n",
    "    p0 = DS['P0'].values[0]\n",
    "    \n",
    "    plevm = DS['hyam']*p0 + DS['hybm']*DS['PS'].isel(lat=0,lon=0) # Mid level\n",
    "    plevm.attrs['units'] = \"Pa\"\n",
    "\n",
    "    # Height with standard atmosphere\n",
    "    zlevm      = plevm\n",
    "    zlevm_vals = 1000.*np.asarray(mpc.pressure_to_height_std(plevm)) # Units of [m] after multiplied \n",
    "    zlevm      = plevm.copy(deep=True)\n",
    "    zlevm[:,:] = zlevm_vals\n",
    "    \n",
    "    pvar        = DS[pbl_var].isel(lat=0,lon=0)\n",
    "    pvar['lev'] = zlevm[0,:].values\n",
    "    dvardz      = pvar.differentiate(\"lev\") # Find field gradient wrt HEIGHT!\n",
    "\n",
    "    dvardz.loc[:,200:]   = 0.  # Restrict to a specificheight region\n",
    "    dvardz.loc[:,:3000.] = 0\n",
    "\n",
    "    nT = np.shape(dvardz)[0]\n",
    "    PBLdepth = np.full([nT], np.nan)\n",
    "\n",
    "    for iT in range(nT):\n",
    "        iLevs  = np.where((zlevm[iT,:]>=200) & (zlevm[iT,:]<=3000))[0]\n",
    "        maxLev = np.where(dvardz[iT,iLevs]==np.nanmax(dvardz[iT,iLevs]))[0]\n",
    "        PBLdepth[iT] = zlevm[iT,iLevs[maxLev[0]]]\n",
    "    \n",
    "    return PBLdepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9adb7e1-ce48-4e4d-81b4-843ff5dbc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function to get the height of the PBL as the level with maximum d(var)/dz. \n",
    "Inputs:  A dataset with CAM output ('DS') and the variable to differentiate ('pbl_var')\n",
    "Outputs: An array with boundary layer depth\n",
    "'''\n",
    "def PBLHasMaxDZ_abs(DS, pbl_var): \n",
    "    # Convert HMGcamDS_all to height (nabbed from Rich's script)\n",
    "    p0 = DS['P0'].values[0]\n",
    "    \n",
    "    plevm = DS['hyam']*p0 + DS['hybm']*DS['PS'].isel(lat=0,lon=0) # Mid level\n",
    "    plevm.attrs['units'] = \"Pa\"\n",
    "\n",
    "    # Height with standard atmosphere\n",
    "    zlevm      = plevm\n",
    "    zlevm_vals = 1000.*np.asarray(mpc.pressure_to_height_std(plevm)) # Units of [m] after multiplied \n",
    "    zlevm      = plevm.copy(deep=True)\n",
    "    zlevm[:,:] = zlevm_vals\n",
    "    \n",
    "    pvar        = DS[pbl_var].isel(lat=0,lon=0)\n",
    "    pvar['lev'] = zlevm[0,:].values\n",
    "    dvardz      = pvar.differentiate(\"lev\") # Find field gradient wrt HEIGHT!\n",
    "\n",
    "    dvardz.loc[:,200:]   = 0.  # Restrict to a specificheight region\n",
    "    dvardz.loc[:,:3000.] = 0\n",
    "\n",
    "    nT = np.shape(dvardz)[0]\n",
    "    PBLdepth = np.full([nT], np.nan)\n",
    "\n",
    "    for iT in range(nT):\n",
    "        iLevs  = np.where((zlevm[iT,:]>=200) & (zlevm[iT,:]<=3000))[0]\n",
    "        maxLev = np.where(np.abs(dvardz[iT,iLevs])==np.nanmax(np.abs(dvardz[iT,iLevs])))[0]\n",
    "        PBLdepth[iT] = zlevm[iT,iLevs[maxLev[0]]]\n",
    "    \n",
    "    return PBLdepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9e0cc1-705d-4447-9bd7-fe1afd608cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateToPressure_v2(DS, varName, pressGoals):\n",
    "#     nCases = len(DSin.case.values)\n",
    "#     nTimes = len(DSin.time.values)\n",
    "    \n",
    "#     saveOut = np.full([nTimes,len(pressGoals),1,1], np.nan)\n",
    "\n",
    "    ## For the larger arrays, need to operate case-by-case; input to vinth2p can only be 3 or 4 dimensions. \n",
    "#     for iCase in range(nCases): \n",
    "#     DS = DSin\n",
    "\n",
    "    p0mb = DS.P0.values[0]/100        # mb\n",
    "\n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = DS.hyam.values[0,:]\n",
    "    hybm = DS.hybm.values[0,:]\n",
    "    hyai = DS.hyai.values[0,:]\n",
    "    hybi = DS.hybi.values[0,:]\n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DS.PS.values              # Pa \n",
    "\n",
    "    # Converting variables: \n",
    "    if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "\n",
    "    saveOut = varInterp\n",
    "    \n",
    "    return saveOut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc2c67-03d5-4208-8b52-eb48ccf6ace6",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fadebd50-15d7-4011-a843-cb6aa16c8668",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with day 0 of 66 \n",
      "Done with day 1 of 66 \n",
      "Done with day 2 of 66 \n",
      "Done with day 3 of 66 \n",
      "Done with day 4 of 66 \n",
      "Done with day 5 of 66 \n",
      "Done with day 6 of 66 \n",
      "Done with day 7 of 66 \n",
      "Done with day 8 of 66 \n",
      "Done with day 9 of 66 \n",
      "Done with day 10 of 66 \n",
      "Done with day 11 of 66 \n",
      "Done with day 12 of 66 \n",
      "Done with day 13 of 66 \n",
      "Done with day 14 of 66 \n",
      "Done with day 15 of 66 \n",
      "Done with day 16 of 66 \n",
      "Done with day 17 of 66 \n",
      "Done with day 18 of 66 \n",
      "Done with day 19 of 66 \n",
      "Done with day 20 of 66 \n",
      "Done with day 21 of 66 \n",
      "Done with day 22 of 66 \n",
      "Done with day 23 of 66 \n",
      "Done with day 24 of 66 \n",
      "Done with day 25 of 66 \n",
      "Done with day 26 of 66 \n",
      "Done with day 27 of 66 \n",
      "Done with day 28 of 66 \n",
      "Done with day 29 of 66 \n",
      "Done with day 30 of 66 \n",
      "Done with day 31 of 66 \n",
      "Done with day 32 of 66 \n",
      "Done with day 33 of 66 \n",
      "Done with day 34 of 66 \n",
      "Done with day 35 of 66 \n",
      "Done with day 36 of 66 \n",
      "Done with day 37 of 66 \n",
      "Done with day 38 of 66 \n",
      "Done with day 39 of 66 \n",
      "Done with day 40 of 66 \n",
      "Done with day 41 of 66 \n",
      "Done with day 42 of 66 \n",
      "Done with day 43 of 66 \n",
      "Done with day 44 of 66 \n",
      "Done with day 45 of 66 \n",
      "Done with day 46 of 66 \n",
      "Done with day 47 of 66 \n",
      "Done with day 48 of 66 \n",
      "Done with day 49 of 66 \n",
      "Done with day 50 of 66 \n",
      "Done with day 51 of 66 \n",
      "Done with day 52 of 66 \n",
      "Done with day 53 of 66 \n",
      "Done with day 54 of 66 \n",
      "Done with day 55 of 66 \n",
      "Done with day 56 of 66 \n",
      "Done with day 57 of 66 \n",
      "Done with day 58 of 66 \n",
      "Done with day 59 of 66 \n",
      "Done with day 60 of 66 \n",
      "Done with day 61 of 66 \n",
      "Done with day 62 of 66 \n",
      "Done with day 63 of 66 \n",
      "Done with day 64 of 66 \n",
      "Done with day 65 of 66 \n",
      "Done with pertlim  0\n"
     ]
    }
   ],
   "source": [
    "## Where files are saved + start of casename\n",
    "# dataDir = '/glade/scratch/mdfowler/CLASP_ensOutput/'\n",
    "dataDir = '/glade/scratch/mdfowler/CLASP_ensOutput/multiplier/mult100_2017/'\n",
    "\n",
    "htgFileStart = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTGens'\n",
    "# hmgFileStart = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HMGens'\n",
    "\n",
    "## Extra vars I want, but they're on the h0 files instead of the h1 files (same time step, just different stream) \n",
    "addVars = np.asarray(['SWCF','LWCF','TS','CLOUD','FSNS','FLNS','PS','QREFHT',\n",
    "                      'U10','CLDHGH','CLDLIQ','TMQ'])\n",
    "\n",
    "## Other settings to build up case names \n",
    "pertVals = np.asarray(['00'])\n",
    "\n",
    "\n",
    "pertCount = 0\n",
    "for iPert in range(len(pertVals)): \n",
    "    \n",
    "    ## **** IMPORTANT: I've added the 2016 option to the listed files to look at just one year right now\n",
    "    \n",
    "    # listFilesHMG_camh1 = np.sort(glob.glob(dataDir+hmgFileStart+'*pert'+pertVals[iPert]+'*cam.h1*'))\n",
    "    listFilesHTG_camh1 = np.sort(glob.glob(dataDir+htgFileStart+'*pert'+pertVals[iPert]+'*cam.h1*'))\n",
    "    \n",
    "    # listFilesHMG_camh0 = np.sort(glob.glob(dataDir+hmgFileStart+'*pert'+pertVals[iPert]+'*cam.h0*'))\n",
    "    listFilesHTG_camh0 = np.sort(glob.glob(dataDir+htgFileStart+'*pert'+pertVals[iPert]+'*cam.h0*'))\n",
    "    \n",
    "    # listFilesHMG_clmh0 = np.sort(glob.glob(dataDir+hmgFileStart+'*pert'+pertVals[iPert]+'*clm2.h0.*'))\n",
    "    listFilesHTG_clmh0 = np.sort(glob.glob(dataDir+htgFileStart+'*pert'+pertVals[iPert]+'*clm2.h0.*'))\n",
    "    \n",
    "    fileCount=0\n",
    "    for iFile in range(len(listFilesHTG_camh1)):\n",
    "        # with xr.open_dataset(listFilesHMG_camh1[iFile], decode_times=False) as hmgDS: \n",
    "        #     hmgDS         = cesm_correct_time(hmgDS)\n",
    "        #     hmgDS['time'] = hmgDS.indexes['time'].to_datetimeindex() \n",
    "        with xr.open_dataset(listFilesHTG_camh1[iFile], decode_times=False) as htgDS: \n",
    "            htgDS         = cesm_correct_time(htgDS)\n",
    "            htgDS['time'] = htgDS.indexes['time'].to_datetimeindex()\n",
    "\n",
    "        # with xr.open_dataset(listFilesHMG_clmh0[iFile], decode_times=True) as clm_hmgDS: \n",
    "        #     clm_hmgDS['time'] = hmgDS['time']\n",
    "        with xr.open_dataset(listFilesHTG_clmh0[iFile], decode_times=True) as clm_htgDS: \n",
    "            clm_htgDS['time'] = htgDS['time']    \n",
    "            \n",
    "        \n",
    "        ## Add variables as needed (some are on h0 file stream)\n",
    "        for iVar in range(len(addVars)):\n",
    "            # varHMG         = xr.open_dataset(listFilesHMG_camh0[iFile])[addVars[iVar]]\n",
    "            # varHMG['time'] = hmgDS['time']\n",
    "\n",
    "            varHTG         = xr.open_dataset(listFilesHTG_camh0[iFile])[addVars[iVar]]\n",
    "            varHTG['time'] = htgDS['time']\n",
    "\n",
    "            # hmgDS = xr.merge([hmgDS, varHMG])\n",
    "            htgDS = xr.merge([htgDS, varHTG])\n",
    "\n",
    "        iTimeStart_day1  = np.where(  htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(1,'D')) )[0]\n",
    "        iTimeStart_day2  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(2,'D'))))[0]\n",
    "        iTimeStart_day3  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(2,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(3,'D'))))[0]\n",
    "        iTimeStart_day4  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(3,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(4,'D'))))[0]\n",
    "        iTimeStart_day5  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(4,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(5,'D'))))[0]\n",
    "        iTimeStart_day6  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(5,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(6,'D'))))[0]\n",
    "        \n",
    "\n",
    "        # Build larger array \n",
    "        if fileCount==0:\n",
    "#             hmg_allDay1      = hmgDS.isel(time=iTimeStart_day1)\n",
    "#             hmg_allDay2      = hmgDS.isel(time=iTimeStart_day2)\n",
    "#             hmg_allDay3      = hmgDS.isel(time=iTimeStart_day3)\n",
    "#             hmg_allDay4      = hmgDS.isel(time=iTimeStart_day4)\n",
    "#             hmg_allDay5      = hmgDS.isel(time=iTimeStart_day5)\n",
    "#             hmg_allDay6      = hmgDS.isel(time=iTimeStart_day6)\n",
    "            \n",
    "            # htg_allDay1      = htgDS.isel(time=iTimeStart_day1)\n",
    "            htg_allDay2      = htgDS.isel(time=iTimeStart_day2)\n",
    "            htg_allDay3      = htgDS.isel(time=iTimeStart_day3)\n",
    "            htg_allDay4      = htgDS.isel(time=iTimeStart_day4)\n",
    "            htg_allDay5      = htgDS.isel(time=iTimeStart_day5)\n",
    "            htg_allDay6      = htgDS.isel(time=iTimeStart_day6)\n",
    "            \n",
    "            # clmHMG_allDay1   = clm_hmgDS.isel(time=iTimeStart_day1)\n",
    "#             clmHMG_allDay2   = clm_hmgDS.isel(time=iTimeStart_day2)\n",
    "#             clmHMG_allDay3   = clm_hmgDS.isel(time=iTimeStart_day3)\n",
    "#             clmHMG_allDay4   = clm_hmgDS.isel(time=iTimeStart_day4)\n",
    "#             clmHMG_allDay5   = clm_hmgDS.isel(time=iTimeStart_day5)\n",
    "#             clmHMG_allDay6   = clm_hmgDS.isel(time=iTimeStart_day6)\n",
    "        \n",
    "            # clmHTG_allDay1   = clm_htgDS.isel(time=iTimeStart_day1)\n",
    "            clmHTG_allDay2   = clm_htgDS.isel(time=iTimeStart_day2)\n",
    "            clmHTG_allDay3   = clm_htgDS.isel(time=iTimeStart_day3)\n",
    "            clmHTG_allDay4   = clm_htgDS.isel(time=iTimeStart_day4)\n",
    "            clmHTG_allDay5   = clm_htgDS.isel(time=iTimeStart_day5)\n",
    "            clmHTG_allDay6   = clm_htgDS.isel(time=iTimeStart_day6)\n",
    "            \n",
    "        else: \n",
    "#             hmg_allDay1      = xr.concat([hmg_allDay1,    hmgDS.isel(time=iTimeStart_day1)],  dim='time')\n",
    "#             hmg_allDay2      = xr.concat([hmg_allDay2,    hmgDS.isel(time=iTimeStart_day2)],  dim='time')\n",
    "#             hmg_allDay3      = xr.concat([hmg_allDay3,    hmgDS.isel(time=iTimeStart_day3)],  dim='time')\n",
    "#             hmg_allDay4      = xr.concat([hmg_allDay4,    hmgDS.isel(time=iTimeStart_day4)],  dim='time')\n",
    "#             hmg_allDay5      = xr.concat([hmg_allDay5,    hmgDS.isel(time=iTimeStart_day5)],  dim='time')\n",
    "#             hmg_allDay6      = xr.concat([hmg_allDay6,    hmgDS.isel(time=iTimeStart_day6)],  dim='time')\n",
    "            \n",
    "#             htg_allDay1      = xr.concat([htg_allDay1,    htgDS.isel(time=iTimeStart_day1)],  dim='time')\n",
    "            htg_allDay2      = xr.concat([htg_allDay2,    htgDS.isel(time=iTimeStart_day2)],  dim='time')\n",
    "            htg_allDay3      = xr.concat([htg_allDay3,    htgDS.isel(time=iTimeStart_day3)],  dim='time')\n",
    "            htg_allDay4      = xr.concat([htg_allDay4,    htgDS.isel(time=iTimeStart_day4)],  dim='time')\n",
    "            htg_allDay5      = xr.concat([htg_allDay5,    htgDS.isel(time=iTimeStart_day5)],  dim='time')\n",
    "            htg_allDay6      = xr.concat([htg_allDay6,    htgDS.isel(time=iTimeStart_day6)],  dim='time')   \n",
    "            \n",
    "            \n",
    "#             clmHMG_allDay1   = xr.concat([clmHMG_allDay1, clm_hmgDS.isel(time=iTimeStart_day1)], dim='time', data_vars='minimal')\n",
    "#             clmHMG_allDay2   = xr.concat([clmHMG_allDay2, clm_hmgDS.isel(time=iTimeStart_day2)], dim='time', data_vars='minimal')\n",
    "#             clmHMG_allDay3   = xr.concat([clmHMG_allDay3, clm_hmgDS.isel(time=iTimeStart_day3)], dim='time', data_vars='minimal')\n",
    "#             clmHMG_allDay4   = xr.concat([clmHMG_allDay4, clm_hmgDS.isel(time=iTimeStart_day4)], dim='time', data_vars='minimal')\n",
    "#             clmHMG_allDay5   = xr.concat([clmHMG_allDay5, clm_hmgDS.isel(time=iTimeStart_day5)], dim='time', data_vars='minimal')\n",
    "#             clmHMG_allDay6   = xr.concat([clmHMG_allDay6, clm_hmgDS.isel(time=iTimeStart_day6)], dim='time', data_vars='minimal')\n",
    "            \n",
    "#             clmHTG_allDay1   = xr.concat([clmHTG_allDay1, clm_htgDS.isel(time=iTimeStart_day1)], dim='time', data_vars='minimal')\n",
    "            clmHTG_allDay2   = xr.concat([clmHTG_allDay2, clm_htgDS.isel(time=iTimeStart_day2)], dim='time', data_vars='minimal')\n",
    "            clmHTG_allDay3   = xr.concat([clmHTG_allDay3, clm_htgDS.isel(time=iTimeStart_day3)], dim='time', data_vars='minimal')\n",
    "            clmHTG_allDay4   = xr.concat([clmHTG_allDay4, clm_htgDS.isel(time=iTimeStart_day4)], dim='time', data_vars='minimal')\n",
    "            clmHTG_allDay5   = xr.concat([clmHTG_allDay5, clm_htgDS.isel(time=iTimeStart_day5)], dim='time', data_vars='minimal')\n",
    "            clmHTG_allDay6   = xr.concat([clmHTG_allDay6, clm_htgDS.isel(time=iTimeStart_day6)], dim='time', data_vars='minimal')\n",
    "            \n",
    "        fileCount = fileCount+1\n",
    "        print('Done with day %i of %i ' % (iFile, len(listFilesHTG_camh1)) )\n",
    "            \n",
    "    ## Combine into larger HTG or HMG arrays for all pertlim experiments \n",
    "#     hmg_allDay1  = hmg_allDay1.assign_coords({\"pertlim\": iPert})\n",
    "#     hmg_allDay2  = hmg_allDay2.assign_coords({\"pertlim\": iPert})\n",
    "#     hmg_allDay3  = hmg_allDay3.assign_coords({\"pertlim\": iPert})\n",
    "#     hmg_allDay4  = hmg_allDay4.assign_coords({\"pertlim\": iPert})\n",
    "#     hmg_allDay5  = hmg_allDay5.assign_coords({\"pertlim\": iPert})\n",
    "#     hmg_allDay6  = hmg_allDay6.assign_coords({\"pertlim\": iPert})\n",
    "    \n",
    "#     htg_allDay1  = htg_allDay1.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay2  = htg_allDay2.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay3  = htg_allDay3.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay4  = htg_allDay4.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay5  = htg_allDay5.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay6  = htg_allDay6.assign_coords({\"pertlim\": iPert})\n",
    "    \n",
    "#     clmHMG_allDay1 = clmHMG_allDay1.assign_coords({\"pertlim\": iPert})\n",
    "#     clmHMG_allDay2 = clmHMG_allDay2.assign_coords({\"pertlim\": iPert})\n",
    "#     clmHMG_allDay3 = clmHMG_allDay3.assign_coords({\"pertlim\": iPert})\n",
    "#     clmHMG_allDay4 = clmHMG_allDay4.assign_coords({\"pertlim\": iPert})\n",
    "#     clmHMG_allDay5 = clmHMG_allDay5.assign_coords({\"pertlim\": iPert})\n",
    "#     clmHMG_allDay6 = clmHMG_allDay6.assign_coords({\"pertlim\": iPert})\n",
    "    \n",
    "#     clmHTG_allDay1 = clmHTG_allDay1.assign_coords({\"pertlim\": iPert})\n",
    "    clmHTG_allDay2 = clmHTG_allDay2.assign_coords({\"pertlim\": iPert})\n",
    "    clmHTG_allDay3 = clmHTG_allDay3.assign_coords({\"pertlim\": iPert})\n",
    "    clmHTG_allDay4 = clmHTG_allDay4.assign_coords({\"pertlim\": iPert})\n",
    "    clmHTG_allDay5 = clmHTG_allDay5.assign_coords({\"pertlim\": iPert})\n",
    "    clmHTG_allDay6 = clmHTG_allDay6.assign_coords({\"pertlim\": iPert})\n",
    "  \n",
    "\n",
    "    if pertCount==0: \n",
    "#         hmgDS_pertDay1 = hmg_allDay1\n",
    "#         hmgDS_pertDay2 = hmg_allDay2\n",
    "#         hmgDS_pertDay3 = hmg_allDay3\n",
    "#         hmgDS_pertDay4 = hmg_allDay4\n",
    "#         hmgDS_pertDay5 = hmg_allDay5\n",
    "#         hmgDS_pertDay6 = hmg_allDay6\n",
    "        \n",
    "#         htgDS_pertDay1 = htg_allDay1\n",
    "        htgDS_pertDay2 = htg_allDay2\n",
    "        htgDS_pertDay3 = htg_allDay3\n",
    "        htgDS_pertDay4 = htg_allDay4\n",
    "        htgDS_pertDay5 = htg_allDay5\n",
    "        htgDS_pertDay6 = htg_allDay6\n",
    "        \n",
    "#         clm_hmgDS_pertDay1 = clmHMG_allDay1\n",
    "#         clm_hmgDS_pertDay2 = clmHMG_allDay2\n",
    "#         clm_hmgDS_pertDay3 = clmHMG_allDay3\n",
    "#         clm_hmgDS_pertDay4 = clmHMG_allDay4\n",
    "#         clm_hmgDS_pertDay5 = clmHMG_allDay5\n",
    "#         clm_hmgDS_pertDay6 = clmHMG_allDay6\n",
    "        \n",
    "#         clm_htgDS_pertDay1 = clmHTG_allDay1\n",
    "        clm_htgDS_pertDay2 = clmHTG_allDay2\n",
    "        clm_htgDS_pertDay3 = clmHTG_allDay3\n",
    "        clm_htgDS_pertDay4 = clmHTG_allDay4\n",
    "        clm_htgDS_pertDay5 = clmHTG_allDay5\n",
    "        clm_htgDS_pertDay6 = clmHTG_allDay6\n",
    "        \n",
    "        \n",
    " \n",
    "    else: \n",
    "#         hmgDS_pertDay1 = xr.concat([hmgDS_pertDay1, hmg_allDay1], \"pertlim\")\n",
    "#         hmgDS_pertDay2 = xr.concat([hmgDS_pertDay2, hmg_allDay2], \"pertlim\")\n",
    "#         hmgDS_pertDay3 = xr.concat([hmgDS_pertDay3, hmg_allDay3], \"pertlim\")\n",
    "#         hmgDS_pertDay4 = xr.concat([hmgDS_pertDay4, hmg_allDay4], \"pertlim\")\n",
    "#         hmgDS_pertDay5 = xr.concat([hmgDS_pertDay5, hmg_allDay5], \"pertlim\")\n",
    "#         hmgDS_pertDay6 = xr.concat([hmgDS_pertDay6, hmg_allDay6], \"pertlim\")\n",
    "        \n",
    "#         htgDS_pertDay1 = xr.concat([htgDS_pertDay1, htg_allDay1], \"pertlim\")\n",
    "        htgDS_pertDay2 = xr.concat([htgDS_pertDay2, htg_allDay2], \"pertlim\")\n",
    "        htgDS_pertDay3 = xr.concat([htgDS_pertDay3, htg_allDay3], \"pertlim\")\n",
    "        htgDS_pertDay4 = xr.concat([htgDS_pertDay4, htg_allDay4], \"pertlim\")\n",
    "        htgDS_pertDay5 = xr.concat([htgDS_pertDay5, htg_allDay5], \"pertlim\")\n",
    "        htgDS_pertDay6 = xr.concat([htgDS_pertDay6, htg_allDay6], \"pertlim\")\n",
    "        \n",
    "#         clm_hmgDS_pertDay1 = xr.concat([clm_hmgDS_pertDay1, clmHMG_allDay1], \"pertlim\")\n",
    "#         clm_hmgDS_pertDay2 = xr.concat([clm_hmgDS_pertDay2, clmHMG_allDay2], \"pertlim\")\n",
    "#         clm_hmgDS_pertDay3 = xr.concat([clm_hmgDS_pertDay3, clmHMG_allDay3], \"pertlim\")\n",
    "#         clm_hmgDS_pertDay4 = xr.concat([clm_hmgDS_pertDay4, clmHMG_allDay4], \"pertlim\")\n",
    "#         clm_hmgDS_pertDay5 = xr.concat([clm_hmgDS_pertDay5, clmHMG_allDay5], \"pertlim\")\n",
    "#         clm_hmgDS_pertDay6 = xr.concat([clm_hmgDS_pertDay6, clmHMG_allDay6], \"pertlim\")\n",
    "\n",
    "#         clm_htgDS_pertDay1 = xr.concat([clm_htgDS_pertDay1, clmHTG_allDay1], \"pertlim\")\n",
    "        clm_htgDS_pertDay2 = xr.concat([clm_htgDS_pertDay2, clmHTG_allDay2], \"pertlim\")\n",
    "        clm_htgDS_pertDay3 = xr.concat([clm_htgDS_pertDay3, clmHTG_allDay3], \"pertlim\")\n",
    "        clm_htgDS_pertDay4 = xr.concat([clm_htgDS_pertDay4, clmHTG_allDay4], \"pertlim\")\n",
    "        clm_htgDS_pertDay5 = xr.concat([clm_htgDS_pertDay5, clmHTG_allDay5], \"pertlim\")\n",
    "        clm_htgDS_pertDay6 = xr.concat([clm_htgDS_pertDay6, clmHTG_allDay6], \"pertlim\")\n",
    "\n",
    "\n",
    "#     del hmg_allDay1\n",
    "#     del hmg_allDay2\n",
    "#     del hmg_allDay3\n",
    "#     del hmg_allDay4\n",
    "#     del hmg_allDay5\n",
    "#     del hmg_allDay6\n",
    "    \n",
    "#     del htg_allDay1\n",
    "#     del htg_allDay2\n",
    "#     del htg_allDay3\n",
    "#     del htg_allDay4\n",
    "#     del htg_allDay5\n",
    "#     del htg_allDay6    \n",
    "\n",
    "    \n",
    "    print('Done with pertlim ', pertCount)\n",
    "    \n",
    "    pertCount=pertCount+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf72ab56-5dc7-4fe2-82af-0d4d450a4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,axs = plt.subplots(1,1, figsize=(18,6) )\n",
    "\n",
    "# varPlot = 'PBLH'\n",
    "\n",
    "# plotHMG  = False\n",
    "# plotDiff = False\n",
    "\n",
    "# ## Plot just the HMG values \n",
    "# if plotHMG==True:\n",
    "#     axs.plot(hmgDS_pertDay1.time.values, np.squeeze(hmgDS_pertDay1[varPlot].values), color='firebrick',  label='HMG_day1')\n",
    "#     axs.plot(hmgDS_pertDay2.time.values, np.squeeze(hmgDS_pertDay2[varPlot].values), color='darkorange', label='HMG_day2')\n",
    "#     axs.plot(hmgDS_pertDay3.time.values, np.squeeze(hmgDS_pertDay3[varPlot].values), color='olivedrab',  label='HMG_day3')\n",
    "#     axs.plot(hmgDS_pertDay4.time.values, np.squeeze(hmgDS_pertDay4[varPlot].values), color='turquoise',  label='HMG_day4')\n",
    "#     axs.plot(hmgDS_pertDay5.time.values, np.squeeze(hmgDS_pertDay5[varPlot].values), color='royalblue',  label='HMG_day5')\n",
    "#     axs.plot(hmgDS_pertDay6.time.values, np.squeeze(hmgDS_pertDay6[varPlot].values), color='blueviolet', label='HMG_day6')\n",
    "#     axs.set_title('HMG '+varPlot+' (by lead day)')\n",
    "\n",
    "# ## If you'd rather plot differences....\n",
    "# if plotDiff==True:\n",
    "#     axs.plot(hmgDS_pertDay1.time.values, np.squeeze(htgDS_pertDay1[varPlot].values-hmgDS_pertDay1[varPlot].values), color='firebrick',  label='HTG-HMG_day1')\n",
    "#     axs.plot(hmgDS_pertDay2.time.values, np.squeeze(htgDS_pertDay2[varPlot].values-hmgDS_pertDay2[varPlot].values), color='darkorange', label='HTG-HMG_day2')\n",
    "#     axs.plot(hmgDS_pertDay3.time.values, np.squeeze(htgDS_pertDay3[varPlot].values-hmgDS_pertDay3[varPlot].values), color='olivedrab',  label='HTG-HMG_day3')\n",
    "#     axs.plot(hmgDS_pertDay4.time.values, np.squeeze(htgDS_pertDay4[varPlot].values-hmgDS_pertDay4[varPlot].values), color='turquoise',  label='HTG-HMG_day4')\n",
    "#     axs.plot(hmgDS_pertDay5.time.values, np.squeeze(htgDS_pertDay5[varPlot].values-hmgDS_pertDay5[varPlot].values), color='royalblue',  label='HTG-HMG_day5')\n",
    "#     axs.plot(hmgDS_pertDay6.time.values, np.squeeze(htgDS_pertDay6[varPlot].values-hmgDS_pertDay6[varPlot].values), color='blueviolet', label='HTG-HMG_day6')\n",
    "#     axs.set_title('HTG-HMG '+varPlot+' (by lead day)')\n",
    "\n",
    "\n",
    "# axs.legend()\n",
    "# axs.set_xlim([datetime.date(2015,6,9), datetime.date(2015,6,25)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528c72a1-cf21-4312-86fa-acc962551229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First four times in UTC:\n",
      " ['2017-06-03T05:03:30.000000000' '2017-06-03T05:08:30.000000000'\n",
      " '2017-06-03T05:13:30.000000000' '2017-06-03T05:18:30.000000000'\n",
      " '2017-06-03T05:23:30.000000000']\n",
      "First four times in local:\n",
      " ['2017-06-03T00:03:30.000000000' '2017-06-03T00:08:30.000000000'\n",
      " '2017-06-03T00:13:30.000000000' '2017-06-03T00:18:30.000000000'\n",
      " '2017-06-03T00:23:30.000000000']\n"
     ]
    }
   ],
   "source": [
    "## Convert to local times...\n",
    "# - - - - - - - - - - - - - - \n",
    "# hmgDS_pertDay1_local     = hmgDS_pertDay1.copy(deep=True)\n",
    "# hmgDS_pertDay2_local     = hmgDS_pertDay2.copy(deep=True)\n",
    "# hmgDS_pertDay3_local     = hmgDS_pertDay3.copy(deep=True)\n",
    "# hmgDS_pertDay4_local     = hmgDS_pertDay4.copy(deep=True)\n",
    "# hmgDS_pertDay5_local     = hmgDS_pertDay5.copy(deep=True)\n",
    "# hmgDS_pertDay6_local     = hmgDS_pertDay6.copy(deep=True)\n",
    "\n",
    "# htgDS_pertDay1_local     = htgDS_pertDay1.copy(deep=True)\n",
    "htgDS_pertDay2_local     = htgDS_pertDay2.copy(deep=True)\n",
    "htgDS_pertDay3_local     = htgDS_pertDay3.copy(deep=True)\n",
    "htgDS_pertDay4_local     = htgDS_pertDay4.copy(deep=True)\n",
    "htgDS_pertDay5_local     = htgDS_pertDay5.copy(deep=True)\n",
    "htgDS_pertDay6_local     = htgDS_pertDay6.copy(deep=True)\n",
    "\n",
    "# clm_hmgDS_pertDay1_local = clm_hmgDS_pertDay1.copy(deep=True)\n",
    "# clm_hmgDS_pertDay2_local = clm_hmgDS_pertDay2.copy(deep=True)\n",
    "# clm_hmgDS_pertDay3_local = clm_hmgDS_pertDay3.copy(deep=True)\n",
    "# clm_hmgDS_pertDay4_local = clm_hmgDS_pertDay4.copy(deep=True)\n",
    "# clm_hmgDS_pertDay5_local = clm_hmgDS_pertDay5.copy(deep=True)\n",
    "# clm_hmgDS_pertDay6_local = clm_hmgDS_pertDay6.copy(deep=True)\n",
    "\n",
    "# clm_htgDS_pertDay1_local = clm_htgDS_pertDay1.copy(deep=True)\n",
    "clm_htgDS_pertDay2_local = clm_htgDS_pertDay2.copy(deep=True)\n",
    "clm_htgDS_pertDay3_local = clm_htgDS_pertDay3.copy(deep=True)\n",
    "clm_htgDS_pertDay4_local = clm_htgDS_pertDay4.copy(deep=True)\n",
    "clm_htgDS_pertDay5_local = clm_htgDS_pertDay5.copy(deep=True)\n",
    "clm_htgDS_pertDay6_local = clm_htgDS_pertDay6.copy(deep=True)\n",
    "\n",
    "# Confirmed that all the times are identical, so using the same local time arrays\n",
    "# localTimes_day1 = htgDS_pertDay1['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day2 = htgDS_pertDay2['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day3 = htgDS_pertDay3['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day4 = htgDS_pertDay4['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day5 = htgDS_pertDay5['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day6 = htgDS_pertDay6['time'].values - np.timedelta64(5,'h')\n",
    "\n",
    "\n",
    "# Replace time dimension with local time\n",
    "# hmgDS_pertDay1_local     = hmgDS_pertDay1_local.assign_coords({\"time\": localTimes_day1})\n",
    "# hmgDS_pertDay2_local     = hmgDS_pertDay2_local.assign_coords({\"time\": localTimes_day2})\n",
    "# hmgDS_pertDay3_local     = hmgDS_pertDay3_local.assign_coords({\"time\": localTimes_day3})\n",
    "# hmgDS_pertDay4_local     = hmgDS_pertDay4_local.assign_coords({\"time\": localTimes_day4})\n",
    "# hmgDS_pertDay5_local     = hmgDS_pertDay5_local.assign_coords({\"time\": localTimes_day5})\n",
    "# hmgDS_pertDay6_local     = hmgDS_pertDay6_local.assign_coords({\"time\": localTimes_day6})\n",
    "\n",
    "# htgDS_pertDay1_local     = htgDS_pertDay1_local.assign_coords({\"time\": localTimes_day1})\n",
    "htgDS_pertDay2_local     = htgDS_pertDay2_local.assign_coords({\"time\": localTimes_day2})\n",
    "htgDS_pertDay3_local     = htgDS_pertDay3_local.assign_coords({\"time\": localTimes_day3})\n",
    "htgDS_pertDay4_local     = htgDS_pertDay4_local.assign_coords({\"time\": localTimes_day4})\n",
    "htgDS_pertDay5_local     = htgDS_pertDay5_local.assign_coords({\"time\": localTimes_day5})\n",
    "htgDS_pertDay6_local     = htgDS_pertDay6_local.assign_coords({\"time\": localTimes_day6})\n",
    "\n",
    "# clm_hmgDS_pertDay1_local = clm_hmgDS_pertDay1_local.assign_coords({\"time\": localTimes_day1})\n",
    "# clm_hmgDS_pertDay2_local = clm_hmgDS_pertDay2_local.assign_coords({\"time\": localTimes_day2})\n",
    "# clm_hmgDS_pertDay3_local = clm_hmgDS_pertDay3_local.assign_coords({\"time\": localTimes_day3})\n",
    "# clm_hmgDS_pertDay4_local = clm_hmgDS_pertDay4_local.assign_coords({\"time\": localTimes_day4})\n",
    "# clm_hmgDS_pertDay5_local = clm_hmgDS_pertDay5_local.assign_coords({\"time\": localTimes_day5})\n",
    "# clm_hmgDS_pertDay6_local = clm_hmgDS_pertDay6_local.assign_coords({\"time\": localTimes_day6})\n",
    "\n",
    "# clm_htgDS_pertDay1_local = clm_htgDS_pertDay1_local.assign_coords({\"time\": localTimes_day1})\n",
    "clm_htgDS_pertDay2_local = clm_htgDS_pertDay2_local.assign_coords({\"time\": localTimes_day2})\n",
    "clm_htgDS_pertDay3_local = clm_htgDS_pertDay3_local.assign_coords({\"time\": localTimes_day3})\n",
    "clm_htgDS_pertDay4_local = clm_htgDS_pertDay4_local.assign_coords({\"time\": localTimes_day4})\n",
    "clm_htgDS_pertDay5_local = clm_htgDS_pertDay5_local.assign_coords({\"time\": localTimes_day5})\n",
    "clm_htgDS_pertDay6_local = clm_htgDS_pertDay6_local.assign_coords({\"time\": localTimes_day6})\n",
    "\n",
    "\n",
    "print('First four times in UTC:\\n',   clm_htgDS_pertDay2.time.values[0:5])\n",
    "print('First four times in local:\\n', clm_htgDS_pertDay2_local.time.values[0:5])\n",
    "\n",
    "## Clear out some memory \n",
    "# del hmgDS_pertDay1\n",
    "# del hmgDS_pertDay2\n",
    "# del hmgDS_pertDay3\n",
    "# del hmgDS_pertDay4\n",
    "# del hmgDS_pertDay5\n",
    "# del hmgDS_pertDay6\n",
    "\n",
    "# del htgDS_pertDay1\n",
    "# del htgDS_pertDay2\n",
    "# del htgDS_pertDay3\n",
    "# del htgDS_pertDay4\n",
    "# del htgDS_pertDay5\n",
    "# del htgDS_pertDay6\n",
    "\n",
    "# del clm_hmgDS_pertDay1\n",
    "# del clm_hmgDS_pertDay2\n",
    "# del clm_hmgDS_pertDay3\n",
    "# del clm_hmgDS_pertDay4\n",
    "# del clm_hmgDS_pertDay5\n",
    "# del clm_hmgDS_pertDay6\n",
    "\n",
    "# del clm_htgDS_pertDay1\n",
    "# del clm_htgDS_pertDay2\n",
    "# del clm_htgDS_pertDay3\n",
    "# del clm_htgDS_pertDay4\n",
    "# del clm_htgDS_pertDay5\n",
    "# del clm_htgDS_pertDay6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61b753-b144-48b7-99e9-add398914a5c",
   "metadata": {},
   "source": [
    "## Look into just a few particular days... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102d9e42-18a1-40ce-a0ae-015502859035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getPBLdefinitions(DS):\n",
    "#     # So first, get actual pressures \n",
    "#     p0       = DS['P0'].values[0]\n",
    "#     plevm    = DS['hyam']*p0 + DS['hybm']*DS['PS'].isel(lat=0,lon=0) # Mid level\n",
    "#     plevm.attrs['units'] = \"Pa\"\n",
    "    \n",
    "#      # Get *potential* temperature, not just T \n",
    "#     theta = np.asarray(mpc.potential_temperature(plevm * units.pascals, DS['T'] * units.kelvin))\n",
    "#     # Add to existing DS\n",
    "#     DS['theta'] = (('time','lev','lat','lon'), theta)\n",
    "\n",
    "#     # Height with standard atmosphere\n",
    "#     zlevm_vals = 1000.*np.asarray(mpc.pressure_to_height_std(plevm)) # Units of [m] after multiplied \n",
    "#     zlevm      = plevm.copy(deep=True)\n",
    "#     zlevm[:,:] = zlevm_vals\n",
    "\n",
    "#     # Now compute the BL depth and save it to the larger CAM dataset\n",
    "#     PBLdepth = PBLHasMaxDZ(DS, 'theta')\n",
    "#     print('Done computing PBL depth with theta')\n",
    "\n",
    "#     PBLdepth_thetal = PBLHasMaxDZ(DS, 'THETAL')\n",
    "#     print('Done computing PBL depth with THETAL')\n",
    "    \n",
    "#     PBLdepth_qAbs = PBLHasMaxDZ_abs(DS,   'Q')\n",
    "#     print('Done computing PBL depth with Q')\n",
    "    \n",
    "#     PBLdepth_thetaAbs = PBLHasMaxDZ_abs(DS, 'theta')\n",
    "#     print('Done computing PBL depth with theta_abs')\n",
    "    \n",
    "#     PBLdepth_thetalAbs = PBLHasMaxDZ_abs(DS, 'THETAL')\n",
    "#     print('Done computing PBL depth with thetaL_abs')\n",
    "    \n",
    "    \n",
    "#     # Add above to each dataset\n",
    "#     DS['PBLdepth']    = (('time'), PBLdepth)\n",
    "#     DS['PBLdepth_Q']  = (('time'), PBLdepth_qAbs)\n",
    "#     DS['PBLdepth_thetal']  = (('time'), PBLdepth_thetal)\n",
    "#     DS['PBLdepth_thetaAbs']  = (('time'), PBLdepth_thetaAbs)\n",
    "#     DS['PBLdepth_thetalAbs']  = (('time'), PBLdepth_thetalAbs)\n",
    "    \n",
    "#     ## Belated realization that the heights computed are above *sea level* not above ground level. \n",
    "#     #    Need to subtract elevation. \n",
    "#     DS['PBLdepth']   = DS['PBLdepth']  - elevation[0][0] \n",
    "#     DS['PBLdepth_Q'] = DS['PBLdepth_Q']  - elevation[0][0] \n",
    "#     DS['PBLdepth_thetal'] = DS['PBLdepth_thetal']  - elevation[0][0] \n",
    "#     DS['PBLdepth_thetaAbs']  = DS['PBLdepth_thetaAbs']  - elevation[0][0] \n",
    "#     DS['PBLdepth_thetalAbs'] = DS['PBLdepth_thetalAbs']  - elevation[0][0] \n",
    "\n",
    "#     return DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9534558-6aa8-4b37-b6fb-7a4fe90191eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Read in obs file to correct PBLH elevation (to be above ground level) \n",
    "# nateFile  = '/glade/work/mdfowler/CLASP/clasp-htg.bdate.nc'\n",
    "# nateDS    = xr.open_dataset(nateFile, decode_times=True)\n",
    "# elevation = nateDS.alt.values\n",
    "\n",
    "# ## Select the days of interest\n",
    "# iselTimes = np.where( (hmgDS_pertDay1_local['time.month']==7) & (hmgDS_pertDay1_local['time.day']>=14) & (hmgDS_pertDay1_local['time.day']<=18) )[0]\n",
    "\n",
    "# # hmgDS_pertDay2_selDays = hmgDS_pertDay2_local.isel(time=iselTimes)\n",
    "# # htgDS_pertDay2_selDays = htgDS_pertDay2_local.isel(time=iselTimes)\n",
    "\n",
    "# hmgDS_pertDay2_selDays = hmgDS_pertDay2_local.isel(time=iselTimes).resample(time=\"30Min\").mean()\n",
    "# htgDS_pertDay2_selDays = htgDS_pertDay2_local.isel(time=iselTimes).resample(time=\"30Min\").mean()\n",
    "\n",
    "# ## Calculation PBL definitions...\n",
    "# # hmgDS_pertDay2_withPBLdef = getPBLdefinitions(hmgDS_pertDay2_selDays)\n",
    "# # htgDS_pertDay2_withPBLdef = getPBLdefinitions(htgDS_pertDay2_selDays)\n",
    "# hmgDS_pertDay2_withPBLdef_resample = getPBLdefinitions(hmgDS_pertDay2_selDays)\n",
    "# htgDS_pertDay2_withPBLdef_resample = getPBLdefinitions(htgDS_pertDay2_selDays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "136c51bf-fb54-4d6f-8ae4-df69c66fd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# varNames = ['PBLH','PBLdepth','PBLdepth_Q','PBLdepth_thetal','PBLdepth_thetaAbs','PBLdepth_thetalAbs']\n",
    "\n",
    "# fig,axs = plt.subplots(len(varNames),1, figsize=(18,4*len(varNames)))\n",
    "# axs     = axs.ravel()\n",
    "\n",
    "# for iVar in range(len(varNames)):\n",
    "#     axs[iVar].plot(hmgDS_pertDay2_withPBLdef_resample.time.values, np.squeeze(hmgDS_pertDay2_withPBLdef_resample[varNames[iVar]].values), 'r', label='HMG' )\n",
    "#     axs[iVar].plot(htgDS_pertDay2_withPBLdef_resample.time.values, np.squeeze(htgDS_pertDay2_withPBLdef_resample[varNames[iVar]].values), 'b', label='HTG' )\n",
    "\n",
    "#     axs[iVar].legend()\n",
    "#     axs[iVar].set_title(varNames[iVar]+' (30m data)')\n",
    "#     axs[iVar].axvline(datetime.date(2016,7,16), color='grey')\n",
    "#     axs[iVar].axvline(datetime.date(2016,7,17), color='grey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4239d1-23a6-48b3-b02d-6fe0c0ec0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# varNames = ['LHFLX','SHFLX','TREFHT','SWCF','LWCF','CLDTOT','CLDLOW','CLDHGH','PRECT']\n",
    "\n",
    "# fig,axs = plt.subplots(len(varNames),1, figsize=(18,4*len(varNames)))\n",
    "# axs     = axs.ravel()\n",
    "\n",
    "# for iVar in range(len(varNames)):\n",
    "#     axs[iVar].plot(hmgDS_pertDay2_withPBLdef_resample.time.values, np.squeeze(hmgDS_pertDay2_withPBLdef_resample[varNames[iVar]].values), 'r', label='HMG' )\n",
    "#     axs[iVar].plot(htgDS_pertDay2_withPBLdef_resample.time.values, np.squeeze(htgDS_pertDay2_withPBLdef_resample[varNames[iVar]].values), 'b', label='HTG' )\n",
    "\n",
    "#     axs[iVar].legend()\n",
    "#     axs[iVar].set_title(varNames[iVar]+' (30m data)')\n",
    "#     axs[iVar].axvline(datetime.date(2016,7,16), color='grey')\n",
    "#     axs[iVar].axvline(datetime.date(2016,7,17), color='grey')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7afcfbd-721c-4f6d-b012-f655aeea0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# varNames = ['THETAL','theta','Q']\n",
    "\n",
    "# fig,axs = plt.subplots(len(varNames),2, figsize=(18,4*len(varNames)))\n",
    "# axs     = axs.ravel()\n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# nPlot = 0\n",
    "# for iVar in range(len(varNames)):\n",
    "#     ## Select just certain levels:\n",
    "#     iLevs = np.where(hmgDS_pertDay2_withPBLdef_resample.lev.values>=700)[0]\n",
    "    \n",
    "#     HMGvar = hmgDS_pertDay2_withPBLdef_resample.isel(lev=iLevs)[varNames[iVar]].values\n",
    "#     HTGvar = htgDS_pertDay2_withPBLdef_resample.isel(lev=iLevs)[varNames[iVar]].values\n",
    "    \n",
    "#     if varNames[iVar]=='Q':\n",
    "#         levsRaw_min = np.nanmin([np.nanpercentile(HMGvar, 40), np.nanpercentile(HTGvar, 40)])\n",
    "#         levsRaw_max = np.nanmin([np.nanpercentile(HMGvar, 99), np.nanpercentile(HTGvar, 99)])\n",
    "#         levsRaw     = np.arange(levsRaw_min, levsRaw_max, (np.abs(levsRaw_max) - np.abs(levsRaw_min))/100.0)\n",
    "#         cmap_sel = 'viridis'\n",
    "#     else:\n",
    "#         levsRaw_min = np.nanmin([np.nanpercentile(HMGvar, 1), np.nanpercentile(HTGvar, 1)])\n",
    "#         levsRaw_max = np.nanmin([np.nanpercentile(HMGvar, 60), np.nanpercentile(HTGvar, 60)])\n",
    "#         levsRaw     = np.arange(levsRaw_min, levsRaw_max, (np.abs(levsRaw_max) - np.abs(levsRaw_min))/100.0)\n",
    "#         cmap_sel = 'plasma'\n",
    "\n",
    "        \n",
    "    \n",
    "#     axs[nPlot].contourf(hmgDS_pertDay2_withPBLdef_resample.time.values, hmgDS_pertDay2_withPBLdef_resample.lev.values[iLevs], \n",
    "#                                np.squeeze(HMGvar).transpose(), levsRaw,cmap=cmap_sel, extend='both')\n",
    "#     cplot=axs[nPlot+1].contourf(htgDS_pertDay2_withPBLdef_resample.time.values, htgDS_pertDay2_withPBLdef_resample.lev.values[iLevs],\n",
    "#                                np.squeeze(HTGvar).transpose(), levsRaw,cmap=cmap_sel, extend='both')\n",
    "    \n",
    "#     # Add colorbar\n",
    "#     ax_position = axs[nPlot+1].get_position()\n",
    "#     cbar_ax = fig.add_axes([ax_position.x0-0.2, ax_position.y0-0.06, ax_position.width+0.08, 0.02])\n",
    "#     if ((varNames[iVar]=='Q') | (varNames[iVar]=='Q_interp')):\n",
    "#         cbar = plt.colorbar(cplot, orientation='horizontal',cax=cbar_ax,format='%.1e')\n",
    "#         tick_locator = ticker.MaxNLocator(nbins=5)\n",
    "#         cbar.locator = tick_locator\n",
    "#         cbar.update_ticks()\n",
    "#     else:\n",
    "#         cbar = plt.colorbar(cplot, orientation='horizontal',cax=cbar_ax)\n",
    "\n",
    "#     axs[nPlot].set_title(varNames[iVar]+' HMG (30m data)')\n",
    "#     axs[nPlot+1].set_title(varNames[iVar]+' HTG (30m data)')\n",
    "    \n",
    "#     axs[nPlot].axvline(datetime.date(2016,7,16), color='grey')\n",
    "#     axs[nPlot].axvline(datetime.date(2016,7,17), color='grey')\n",
    "#     axs[nPlot+1].axvline(datetime.date(2016,7,16), color='grey')\n",
    "#     axs[nPlot+1].axvline(datetime.date(2016,7,17), color='grey')\n",
    "    \n",
    "#     axs[nPlot].invert_yaxis()\n",
    "#     axs[nPlot+1].invert_yaxis()\n",
    "    \n",
    "    \n",
    "#     nPlot = nPlot+2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20b81314-ebc1-4c1c-8299-b5689f549d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# varNames = ['PBLH','PBLdepth','PBLdepth_Q','PBLdepth_thetal','PBLdepth_thetaAbs','PBLdepth_thetalAbs']\n",
    "\n",
    "# fig,axs = plt.subplots(len(varNames),1, figsize=(18,4*len(varNames)))\n",
    "# axs     = axs.ravel()\n",
    "\n",
    "# for iVar in range(len(varNames)):\n",
    "#     axs[iVar].plot(hmgDS_pertDay2_withPBLdef.time.values, np.squeeze(hmgDS_pertDay2_withPBLdef[varNames[iVar]].values), 'r', label='HMG' )\n",
    "#     axs[iVar].plot(htgDS_pertDay2_withPBLdef.time.values, np.squeeze(htgDS_pertDay2_withPBLdef[varNames[iVar]].values), 'b', label='HTG' )\n",
    "\n",
    "#     axs[iVar].legend()\n",
    "#     axs[iVar].set_title(varNames[iVar])\n",
    "#     axs[iVar].axvline(datetime.date(2016,7,16), color='grey')\n",
    "#     axs[iVar].axvline(datetime.date(2016,7,17), color='grey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fd051-7fbe-4efc-8003-41bad65a189f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a43437-7ede-436a-b993-350b70cd85e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2a93a-4e9f-44fb-ac02-2a0e7ea2a936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02549bb-e818-4e2a-ba47-f2646785dd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7517df5-8d75-439a-ad83-9d6e3663d28b",
   "metadata": {},
   "source": [
    "## Process the data as we've done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb59e289-b313-4838-be82-4f2610e30901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_camData(DS):\n",
    "    ## Add evaporative fraction to DS \n",
    "    ds_EF = DS.LHFLX.values / (DS.LHFLX.values + DS.SHFLX.values)\n",
    "    DS['EvapFraction'] = (('time'), np.squeeze(ds_EF))\n",
    "\n",
    "    ## Define the actual vertical velocity skew, not just the third order moment \n",
    "    skw_W = DS.WP3_CLUBB.values / ((DS.WP2_CLUBB.values)**1.5)\n",
    "    DS['Skw_W'] = (('time','ilev'), np.squeeze(skw_W))\n",
    "\n",
    "    ## Add in TKE \n",
    "    DS['TKE']  = (('time','ilev','lat,','lon'),\n",
    "                       0.5*(DS['UP2_CLUBB']+DS['VP2_CLUBB']+DS['WP2_CLUBB'])) \n",
    "    DS['TKE'].attrs['units']   = 'm2/s2'\n",
    "    DS['TKE'].attrs['long_name']   = 'Turbulent Kinetic Energy'\n",
    "    \n",
    "    ## Add in wind speed \n",
    "    DS['WindMagnitude']  = (('time','lev','lat,','lon'),\n",
    "                                    np.sqrt((DS.U.values**2.0) + (DS.V.values**2.0)) )\n",
    "\n",
    "    DS['WindMagnitude'].attrs['units']   = 'm/s'\n",
    "    DS['WindMagnitude'].attrs['long_name']   = 'Wind speed'\n",
    "    \n",
    "    ## Compute PBL as max gradient of theta \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "    \n",
    "    # So first, get actual pressures \n",
    "    p0       = DS['P0'].values[0]\n",
    "    plevm    = DS['hyam']*p0 + DS['hybm']*DS['PS'].isel(lat=0,lon=0) # Mid level\n",
    "    plevm.attrs['units'] = \"Pa\"\n",
    "    \n",
    "     # Get *potential* temperature, not just T \n",
    "    theta = np.asarray(mpc.potential_temperature(plevm * units.pascals, DS['T'] * units.kelvin))\n",
    "    # Add to existing DS\n",
    "    DS['theta'] = (('time','lev','lat','lon'), theta)\n",
    "\n",
    "    # Height with standard atmosphere\n",
    "    zlevm_vals = 1000.*np.asarray(mpc.pressure_to_height_std(plevm)) # Units of [m] after multiplied \n",
    "    zlevm      = plevm.copy(deep=True)\n",
    "    zlevm[:,:] = zlevm_vals\n",
    "\n",
    "    # Now compute the BL depth and save it to the larger CAM dataset\n",
    "    PBLdepth = PBLHasMaxDZ(DS, 'theta')\n",
    "    print('Done computing PBL depth with theta')\n",
    "\n",
    "    PBLdepth_qAbs = PBLHasMaxDZ_abs(DS,   'Q')\n",
    "    print('Done computing PBL depth with Q')\n",
    "    \n",
    "    # Add above to each dataset\n",
    "    DS['PBLdepth']    = (('time'), PBLdepth)\n",
    "    DS['PBLdepth_Q']  = (('time'), PBLdepth_qAbs)\n",
    "    \n",
    "    ## Belated realization that the heights computed are above *sea level* not above ground level. \n",
    "    #    Need to subtract elevation. \n",
    "    DS['PBLdepth']   = DS['PBLdepth']  - elevation[0][0] \n",
    "    DS['PBLdepth_Q'] = DS['PBLdepth_Q']  - elevation[0][0] \n",
    "\n",
    "    \n",
    "    ## Interpolate to standard levels \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    print('Beginning interpolation...') \n",
    "    \n",
    "    # Decide on levels to interpoalte to and add to larger arrays\n",
    "    pnew64 = np.arange(200.0,980.0,10.0) \n",
    "\n",
    "    DS = DS.assign_coords({\"levInterp\": pnew64})\n",
    "\n",
    "    varSels = np.asarray(['THLP2_CLUBB','RTP2_CLUBB','RTPTHLP_CLUBB','WPRTP_CLUBB','WPTHLP_CLUBB','WP2_CLUBB','UP2_CLUBB',\n",
    "                          'VP2_CLUBB','TKE','U','V','T','Q','OMEGA','RVMTEND_CLUBB','STEND_CLUBB','CLOUD','CLOUDFRAC_CLUBB',\n",
    "                          'UPWP_CLUBB','WP2RTP_CLUBB','THETAL','WindMagnitude'])\n",
    "\n",
    "    for iVar in range(len(varSels)): \n",
    "        varUnits = DS[varSels[iVar]].units\n",
    "        varName  = DS[varSels[iVar]].long_name\n",
    "\n",
    "        # Interpolate variables and add to larger arrays \n",
    "        interpVar_real = interpolateToPressure_v2(DS, varSels[iVar], pnew64)\n",
    "\n",
    "        DS[varSels[iVar]+'_interp']  = (('time','levInterp','lat','lon'), interpVar_real)\n",
    "\n",
    "        ## Assign attibutes \n",
    "        DS[varSels[iVar]+'_interp'].attrs['units']     = varUnits\n",
    "        DS[varSels[iVar]+'_interp'].attrs['long_name'] = varName\n",
    "\n",
    "    return DS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e0ca18-f34b-4736-8555-e8a1c4140fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in obs file to correct PBLH elevation (to be above ground level) \n",
    "nateFile  = '/glade/work/mdfowler/CLASP/clasp-htg.bdate.nc'\n",
    "nateDS    = xr.open_dataset(nateFile, decode_times=True)\n",
    "elevation = nateDS.alt.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a5dd66-2d8a-4478-a44d-6f120496364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done computing PBL depth with theta\n",
      "Done computing PBL depth with Q\n",
      "Beginning interpolation...\n",
      "Done computing PBL depth with theta\n",
      "Done computing PBL depth with Q\n",
      "Beginning interpolation...\n",
      "Done computing PBL depth with theta\n",
      "Done computing PBL depth with Q\n",
      "Beginning interpolation...\n",
      "Done computing PBL depth with theta\n",
      "Done computing PBL depth with Q\n",
      "Beginning interpolation...\n",
      "Done computing PBL depth with theta\n",
      "Done computing PBL depth with Q\n",
      "Beginning interpolation...\n",
      "Done processing HTG DS\n"
     ]
    }
   ],
   "source": [
    "# ## Process data (HMG)\n",
    "# hmgDS_local_processed_day1 = process_camData(hmgDS_pertDay1_local)\n",
    "# hmgDS_local_processed_day2 = process_camData(hmgDS_pertDay2_local)\n",
    "# hmgDS_local_processed_day3 = process_camData(hmgDS_pertDay3_local)\n",
    "# hmgDS_local_processed_day4 = process_camData(hmgDS_pertDay4_local)\n",
    "# hmgDS_local_processed_day5 = process_camData(hmgDS_pertDay5_local)\n",
    "# hmgDS_local_processed_day6 = process_camData(hmgDS_pertDay6_local)\n",
    "# print('Done processing HMG DS') \n",
    "\n",
    "# htgDS_local_processed_day1 = process_camData(htgDS_pertDay1_local)\n",
    "htgDS_local_processed_day2 = process_camData(htgDS_pertDay2_local)\n",
    "htgDS_local_processed_day3 = process_camData(htgDS_pertDay3_local)\n",
    "htgDS_local_processed_day4 = process_camData(htgDS_pertDay4_local)\n",
    "htgDS_local_processed_day5 = process_camData(htgDS_pertDay5_local)\n",
    "htgDS_local_processed_day6 = process_camData(htgDS_pertDay6_local)\n",
    "print('Done processing HTG DS') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0cbbfe-cfa8-4bfa-8802-73fe9f16b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We really don't need all the variables in CLM dataset....\n",
    "keepVars_CLM = ['SOILWATER_10CM','TSOI_10CM','RAIN','FSA','TG','TSA','USTAR','WSTAR','ZETA']\n",
    "\n",
    "# clm_hmgDS_pertDay1_local = clm_hmgDS_pertDay1_local[keepVars_CLM]\n",
    "# clm_hmgDS_pertDay2_local = clm_hmgDS_pertDay2_local[keepVars_CLM]\n",
    "# clm_hmgDS_pertDay3_local = clm_hmgDS_pertDay3_local[keepVars_CLM]\n",
    "# clm_hmgDS_pertDay4_local = clm_hmgDS_pertDay4_local[keepVars_CLM]\n",
    "# clm_hmgDS_pertDay5_local = clm_hmgDS_pertDay5_local[keepVars_CLM]\n",
    "# clm_hmgDS_pertDay6_local = clm_hmgDS_pertDay6_local[keepVars_CLM]\n",
    "\n",
    "# clm_htgDS_pertDay1_local = clm_htgDS_pertDay1_local[keepVars_CLM]\n",
    "clm_htgDS_pertDay2_local = clm_htgDS_pertDay2_local[keepVars_CLM]\n",
    "clm_htgDS_pertDay3_local = clm_htgDS_pertDay3_local[keepVars_CLM]\n",
    "clm_htgDS_pertDay4_local = clm_htgDS_pertDay4_local[keepVars_CLM]\n",
    "clm_htgDS_pertDay5_local = clm_htgDS_pertDay5_local[keepVars_CLM]\n",
    "clm_htgDS_pertDay6_local = clm_htgDS_pertDay6_local[keepVars_CLM]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4380a930-0eec-4091-a1a2-71ee97e85b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save processed datasets to pickle files for easy read-in \n",
    "# saveDir = '/glade/work/mdfowler/CLASP/histData/processedData/ens_byLeadDay/'\n",
    "\n",
    "# # pickle.dump( hmgDS_local_processed_day1,   open( saveDir+\"realSfc_HMG_day1_pert01.p\", \"wb\" ) )\n",
    "# # pickle.dump( hmgDS_local_processed_day2,   open( saveDir+\"realSfc_HMG_day2_pert01.p\", \"wb\" ) )\n",
    "# # pickle.dump( hmgDS_local_processed_day3,   open( saveDir+\"realSfc_HMG_day3_pert01.p\", \"wb\" ) )\n",
    "# # pickle.dump( hmgDS_local_processed_day4,   open( saveDir+\"realSfc_HMG_day4_pert01.p\", \"wb\" ) )\n",
    "# # pickle.dump( hmgDS_local_processed_day5,   open( saveDir+\"realSfc_HMG_day5_pert01.p\", \"wb\" ) )\n",
    "# # pickle.dump( hmgDS_local_processed_day6,   open( saveDir+\"realSfc_HMG_day6_pert01.p\", \"wb\" ) )\n",
    "# # print('Saved processed HMG files to pickle')\n",
    "\n",
    "# pickle.dump( htgDS_local_processed_day1,   open( saveDir+\"realSfc_HTGmult10_2016day1_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day2,   open( saveDir+\"realSfc_HTGmult10_2016day2_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day3,   open( saveDir+\"realSfc_HTGmult10_2016day3_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day4,   open( saveDir+\"realSfc_HTGmult10_2016day4_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day5,   open( saveDir+\"realSfc_HTGmult10_2016day5_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day6,   open( saveDir+\"realSfc_HTGmult10_2016day6_pert00.p\", \"wb\" ) )\n",
    "# print('Saved processed HTG files to pickle')\n",
    "\n",
    "# pickle.dump( clm_htgDS_pertDay1_local,   open( saveDir+\"realSfc_HTGclmMult10_2016day1_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_htgDS_pertDay2_local,   open( saveDir+\"realSfc_HTGclmMult10_2016day2_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_htgDS_pertDay3_local,   open( saveDir+\"realSfc_HTGclmMult10_2016day3_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_htgDS_pertDay4_local,   open( saveDir+\"realSfc_HTGclmMult10_2016day4_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_htgDS_pertDay5_local,   open( saveDir+\"realSfc_HTGclmMult10_2016day5_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_htgDS_pertDay6_local,   open( saveDir+\"realSfc_HTGclmMult10_2016day6_pert00.p\", \"wb\" ) )\n",
    "# print('Saved processed HTG files to pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97bb53-462b-4f6f-8b10-03005182e949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29fcb7-321b-40c7-a04f-cd9a42175a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e404a439-aef9-420c-8aa5-aa856ed424cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Per suggestion here: https://stackoverflow.com/questions/64282393/how-can-i-speed-up-xarray-resample-much-slower-than-pandas-resample \n",
    "# Need to check here to get this working better: https://stackoverflow.com/questions/55474088/only-valid-with-datetimeindex-timedeltaindex-or-periodindex-but-got-an-instanc \n",
    "# '''\n",
    "# def quick_resample(ds): \n",
    "#     df_h = ds.to_dataframe().set_index('time').resample(\"30Min\").mean()  # what we want (quickly), but in Pandas form\n",
    "#     vals = [xr.DataArray(data=df_h[c], dims=['time'], coords={'time':df_h.index}, attrs=ds[c].attrs) for c in df_h.columns]\n",
    "#     ds_h = xr.Dataset(dict(zip(df_h.columns,vals)), attrs=ds.attrs)\n",
    "    \n",
    "#     return ds_h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e8b276-ba8d-4dba-b97e-113d33427b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_h = htgDS_local_processed_day1.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daba47b-499a-4142-9736-aeb788dbb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c6bed-cd18-4786-a1a6-5483901f711c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff76121-b62e-4450-9ce1-221894b960d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bbc257b-5eb6-45c7-a1ee-a58b752fb3a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampled HTG cases (CAM)\n",
      "Upsampled HTG cases (CLM)\n"
     ]
    }
   ],
   "source": [
    "## Upsample to 30m? \n",
    "\n",
    "# hmg_day1_upsample = hmgDS_local_processed_day1.resample(time=\"30Min\").mean()\n",
    "# hmg_day2_upsample = hmgDS_local_processed_day2.resample(time=\"30Min\").mean()\n",
    "# hmg_day3_upsample = hmgDS_local_processed_day3.resample(time=\"30Min\").mean()\n",
    "# hmg_day4_upsample = hmgDS_local_processed_day4.resample(time=\"30Min\").mean()\n",
    "# hmg_day5_upsample = hmgDS_local_processed_day5.resample(time=\"30Min\").mean()\n",
    "# hmg_day6_upsample = hmgDS_local_processed_day6.resample(time=\"30Min\").mean()\n",
    "# print('Upsampled HMG cases (CAM)')\n",
    "\n",
    "\n",
    "# htg_day1_upsample = htgDS_local_processed_day1.resample(time=\"30Min\").mean()\n",
    "htg_day2_upsample = htgDS_local_processed_day2.resample(time=\"30Min\").mean()\n",
    "htg_day3_upsample = htgDS_local_processed_day3.resample(time=\"30Min\").mean()\n",
    "htg_day4_upsample = htgDS_local_processed_day4.resample(time=\"30Min\").mean()\n",
    "htg_day5_upsample = htgDS_local_processed_day5.resample(time=\"30Min\").mean()\n",
    "htg_day6_upsample = htgDS_local_processed_day6.resample(time=\"30Min\").mean()\n",
    "print('Upsampled HTG cases (CAM)')\n",
    "\n",
    "\n",
    "# clm_hmg_day1_upsample = clm_hmgDS_pertDay1_local.resample(time=\"30Min\").mean()\n",
    "# clm_hmg_day2_upsample = clm_hmgDS_pertDay2_local.resample(time=\"30Min\").mean()\n",
    "# clm_hmg_day3_upsample = clm_hmgDS_pertDay3_local.resample(time=\"30Min\").mean()\n",
    "# clm_hmg_day4_upsample = clm_hmgDS_pertDay4_local.resample(time=\"30Min\").mean()\n",
    "# clm_hmg_day5_upsample = clm_hmgDS_pertDay5_local.resample(time=\"30Min\").mean()\n",
    "# clm_hmg_day6_upsample = clm_hmgDS_pertDay6_local.resample(time=\"30Min\").mean()\n",
    "# print('Upsampled HMG cases (CLM)')\n",
    "\n",
    "\n",
    "# clm_htg_day1_upsample = clm_htgDS_pertDay1_local.resample(time=\"30Min\").mean()\n",
    "clm_htg_day2_upsample = clm_htgDS_pertDay2_local.resample(time=\"30Min\").mean()\n",
    "clm_htg_day3_upsample = clm_htgDS_pertDay3_local.resample(time=\"30Min\").mean()\n",
    "clm_htg_day4_upsample = clm_htgDS_pertDay4_local.resample(time=\"30Min\").mean()\n",
    "clm_htg_day5_upsample = clm_htgDS_pertDay5_local.resample(time=\"30Min\").mean()\n",
    "clm_htg_day6_upsample = clm_htgDS_pertDay6_local.resample(time=\"30Min\").mean()\n",
    "print('Upsampled HTG cases (CLM)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e533a-c06e-48ba-9888-f6fa0b46bc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "444b3903-4b64-40c3-bfd8-f37988b3c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed/upsampled HTG files to pickle\n",
      "Saved upsampled CLM_HTG files to pickle\n"
     ]
    }
   ],
   "source": [
    "## Save upsampled (30m) datasets to pickle files for easy read-in \n",
    "saveDir = '/glade/work/mdfowler/CLASP/histData/processedData/ens_byLeadDay/'\n",
    "\n",
    "# pickle.dump( hmg_day1_upsample,   open( saveDir+\"realSfc_HMG_day1_30min_pert01.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day2_upsample,   open( saveDir+\"realSfc_HMG_day2_30min_pert01.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day3_upsample,   open( saveDir+\"realSfc_HMG_day3_30min_pert01.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day4_upsample,   open( saveDir+\"realSfc_HMG_day4_30min_pert01.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day5_upsample,   open( saveDir+\"realSfc_HMG_day5_30min_pert01.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day6_upsample,   open( saveDir+\"realSfc_HMG_day6_30min_pert01.p\", \"wb\" ) )\n",
    "# print('Saved processed/upsampled HMG files to pickle')\n",
    "\n",
    "# pickle.dump( htg_day1_upsample,   open( saveDir+\"realSfc_HTGmult100_2018day1_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day2_upsample,   open( saveDir+\"realSfc_HTGmult100_2017day2_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day3_upsample,   open( saveDir+\"realSfc_HTGmult100_2017day3_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day4_upsample,   open( saveDir+\"realSfc_HTGmult100_2017day4_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day5_upsample,   open( saveDir+\"realSfc_HTGmult100_2017day5_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day6_upsample,   open( saveDir+\"realSfc_HTGmult100_2017day6_30min_pert00.p\", \"wb\" ) )\n",
    "print('Saved processed/upsampled HTG files to pickle')\n",
    "\n",
    "# pickle.dump( clm_hmg_day1_upsample,   open( saveDir+\"realSfc_HMGclm_day1_30min_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_hmg_day2_upsample,   open( saveDir+\"realSfc_HMGclm_day2_30min_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_hmg_day3_upsample,   open( saveDir+\"realSfc_HMGclm_day3_30min_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_hmg_day4_upsample,   open( saveDir+\"realSfc_HMGclm_day4_30min_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_hmg_day5_upsample,   open( saveDir+\"realSfc_HMGclm_day5_30min_pert00.p\", \"wb\" ) )\n",
    "# pickle.dump( clm_hmg_day6_upsample,   open( saveDir+\"realSfc_HMGclm_day6_30min_pert00.p\", \"wb\" ) )\n",
    "# print('Saved upsampled CLM_HMG files to pickle')\n",
    "\n",
    "# pickle.dump( clm_htg_day1_upsample,   open( saveDir+\"realSfc_HTGclmMult5_2018day1_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( clm_htg_day2_upsample,   open( saveDir+\"realSfc_HTGclmMult100_2017day2_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( clm_htg_day3_upsample,   open( saveDir+\"realSfc_HTGclmMult100_2017day3_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( clm_htg_day4_upsample,   open( saveDir+\"realSfc_HTGclmMult100_2017day4_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( clm_htg_day5_upsample,   open( saveDir+\"realSfc_HTGclmMult100_2017day5_30min_pert00.p\", \"wb\" ) )\n",
    "pickle.dump( clm_htg_day6_upsample,   open( saveDir+\"realSfc_HTGclmMult100_2017day6_30min_pert00.p\", \"wb\" ) )\n",
    "print('Saved upsampled CLM_HTG files to pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cad111-b17c-4021-8aa3-220dbf382a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5bed5-e3e6-4bfc-b596-70007c2463ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyn_env]",
   "language": "python",
   "name": "conda-env-pyn_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
