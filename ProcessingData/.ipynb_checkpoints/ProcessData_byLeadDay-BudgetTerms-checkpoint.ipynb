{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d218aa69-63bf-45bc-aa97-53ac1bf0168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "# # Plotting utils \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker \n",
    "import matplotlib.patches as patches\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# Analysis\n",
    "import time\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "import datetime\n",
    "from   datetime import date, timedelta\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "import glob\n",
    "import os \n",
    "\n",
    "import metpy.calc as mpc\n",
    "from metpy.units import units\n",
    "from metpy.plots import Hodograph, SkewT\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Import Ngl with pyn_env active \n",
    "import Ngl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5782be-f596-4d6d-ab55-6369ad8acc2b",
   "metadata": {},
   "source": [
    "**Useful functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2576f112-953e-45fe-98db-c8724990c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from Brian M. to use time midpoints, not end periods\n",
    "def cesm_correct_time(ds):\n",
    "    \"\"\"Given a Dataset, check for time_bnds,\n",
    "       and use avg(time_bnds) to replace the time coordinate.\n",
    "       Purpose is to center the timestamp on the averaging inverval.   \n",
    "       NOTE: ds should have been loaded using `decode_times=False`\n",
    "    \"\"\"\n",
    "    assert 'time_bnds' in ds\n",
    "    assert 'time' in ds\n",
    "    correct_time_values = ds['time_bnds'].mean(dim='nbnd')\n",
    "    # copy any metadata:\n",
    "    correct_time_values.attrs = ds['time'].attrs\n",
    "    ds = ds.assign_coords({\"time\": correct_time_values})\n",
    "    ds = xr.decode_cf(ds)  # decode to datetime objects\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9e0cc1-705d-4447-9bd7-fe1afd608cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateToPressure_v2(DS, varName, pressGoals):\n",
    "#     nCases = len(DSin.case.values)\n",
    "#     nTimes = len(DSin.time.values)\n",
    "    \n",
    "#     saveOut = np.full([nTimes,len(pressGoals),1,1], np.nan)\n",
    "\n",
    "    ## For the larger arrays, need to operate case-by-case; input to vinth2p can only be 3 or 4 dimensions. \n",
    "#     for iCase in range(nCases): \n",
    "#     DS = DSin\n",
    "\n",
    "    p0mb = DS.P0.values[0]/100        # mb\n",
    "    \n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = DS.hyam.values[0,:]\n",
    "    hybm = DS.hybm.values[0,:]\n",
    "    hyai = DS.hyai.values[0,:]\n",
    "    hybi = DS.hybi.values[0,:]\n",
    "\n",
    "    \n",
    "#     tempFile = listFilesHMG_camh0[0]\n",
    "#     dsTemp   = xr.open_dataset(tempFile, decode_times=True)\n",
    "\n",
    "#     hyam = dsTemp.hyam\n",
    "#     hybm = dsTemp.hybm\n",
    "#     hyai = dsTemp.hyai\n",
    "#     hybi = dsTemp.hybi\n",
    "#     p0mb = dsTemp.P0.values/100.0 \n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DS.PS.values              # Pa \n",
    "\n",
    "    # Converting variables: \n",
    "    if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "\n",
    "    saveOut = varInterp\n",
    "    \n",
    "    return saveOut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc2c67-03d5-4208-8b52-eb48ccf6ace6",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fadebd50-15d7-4011-a843-cb6aa16c8668",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with day 0 of 89 \n",
      "Done with day 1 of 89 \n",
      "Done with day 2 of 89 \n",
      "Done with day 3 of 89 \n",
      "Done with day 4 of 89 \n",
      "Done with day 5 of 89 \n",
      "Done with day 6 of 89 \n",
      "Done with day 7 of 89 \n",
      "Done with day 8 of 89 \n",
      "Done with day 9 of 89 \n",
      "Done with day 10 of 89 \n",
      "Done with day 11 of 89 \n",
      "Done with day 12 of 89 \n",
      "Done with day 13 of 89 \n",
      "Done with day 14 of 89 \n",
      "Done with day 15 of 89 \n",
      "Done with day 16 of 89 \n",
      "Done with day 17 of 89 \n",
      "Done with day 18 of 89 \n",
      "Done with day 19 of 89 \n",
      "Done with day 20 of 89 \n",
      "Done with day 21 of 89 \n",
      "Done with day 22 of 89 \n",
      "Done with day 23 of 89 \n",
      "Done with day 24 of 89 \n",
      "Done with day 25 of 89 \n",
      "Done with day 26 of 89 \n",
      "Done with day 27 of 89 \n",
      "Done with day 28 of 89 \n",
      "Done with day 29 of 89 \n",
      "Done with day 30 of 89 \n",
      "Done with day 31 of 89 \n",
      "Done with day 32 of 89 \n",
      "Done with day 33 of 89 \n",
      "Done with day 34 of 89 \n",
      "Done with day 35 of 89 \n",
      "Done with day 36 of 89 \n",
      "Done with day 37 of 89 \n",
      "Done with day 38 of 89 \n",
      "Done with day 39 of 89 \n",
      "Done with day 40 of 89 \n",
      "Done with day 41 of 89 \n",
      "Done with day 42 of 89 \n",
      "Done with day 43 of 89 \n",
      "Done with day 44 of 89 \n",
      "Done with day 45 of 89 \n",
      "Done with day 46 of 89 \n",
      "Done with day 47 of 89 \n",
      "Done with day 48 of 89 \n",
      "Done with day 49 of 89 \n",
      "Done with day 50 of 89 \n",
      "Done with day 51 of 89 \n",
      "Done with day 52 of 89 \n",
      "Done with day 53 of 89 \n",
      "Done with day 54 of 89 \n",
      "Done with day 55 of 89 \n",
      "Done with day 56 of 89 \n",
      "Done with day 57 of 89 \n",
      "Done with day 58 of 89 \n",
      "Done with day 59 of 89 \n",
      "Done with day 60 of 89 \n",
      "Done with day 61 of 89 \n",
      "Done with day 62 of 89 \n",
      "Done with day 63 of 89 \n",
      "Done with day 64 of 89 \n",
      "Done with day 65 of 89 \n",
      "Done with day 66 of 89 \n",
      "Done with day 67 of 89 \n",
      "Done with day 68 of 89 \n",
      "Done with day 69 of 89 \n",
      "Done with day 70 of 89 \n",
      "Done with day 71 of 89 \n",
      "Done with day 72 of 89 \n",
      "Done with day 73 of 89 \n",
      "Done with day 74 of 89 \n",
      "Done with day 75 of 89 \n",
      "Done with day 76 of 89 \n",
      "Done with day 77 of 89 \n",
      "Done with day 78 of 89 \n",
      "Done with day 79 of 89 \n",
      "Done with day 80 of 89 \n",
      "Done with day 81 of 89 \n",
      "Done with day 82 of 89 \n",
      "Done with day 83 of 89 \n",
      "Done with day 84 of 89 \n",
      "Done with day 85 of 89 \n",
      "Done with day 86 of 89 \n",
      "Done with day 87 of 89 \n",
      "Done with day 88 of 89 \n",
      "Done with pertlim  0\n"
     ]
    }
   ],
   "source": [
    "## Where files are saved + start of casename\n",
    "# dataDir = '/glade/scratch/mdfowler/CLASP_ensOutput/'\n",
    "dataDir = '/glade/scratch/mdfowler/CLASP_ensOutput/multiplier/mult10_2018/'\n",
    "\n",
    "htgFileStart = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HTGens'\n",
    "# hmgFileStart = 'FSCAM.T42_T42.CLASP_fullycoupled_FinalOption.onlyThlRt.HMGens'\n",
    "\n",
    "## Extra vars I want, but they're on the h0 files instead of the h1 files (same time step, just different stream) \n",
    "budgetTerms = ['PS','P0','hybm','hyam','hyai','hybi','thlp2','thlp2_bt','thlp2_cl','thlp2_dp1','thlp2_dp2','thlp2_forcing','thlp2_ma','thlp2_mc','thlp2_pd',\n",
    "                         'thlp2_sf','thlp2_ta','thlp2_tp','thlp2_zt', \n",
    "                          'wp2','wp2_ac','wp2_bp','wp2_bt','wp2_cl','wp2_dp1','wp2_dp2','wp2_ma', 'wp2_pd','wp2_pr1','wp2_pr2',\n",
    "                          'wp2_pr3','wp2_sf','wp2_ta','wp2_zt',\n",
    "                          'wp2rcp','wp2rtp','wp2thlp','wp2thvp','wp3_on_wp2','wp3_on_wp2_zt','wprtp',\n",
    "                          'wprtp2','wprtp_ac','wprtp_bp','wprtp_bt','wprtp_cl','wprtp_dp1','wprtp_enter_mfl','wprtp_exit_mfl',\n",
    "                          'wprtp_forcing','wprtp_ma','wprtp_mc','wprtp_mfl','wprtp_mfl_max','wprtp_mfl_min','wprtp_pd',\n",
    "                          'wprtp_pr1','wprtp_pr2','wprtp_pr3','wprtp_sicl','wprtp_ta','wprtp_tp','wprtp_zt',\n",
    "                          'wprtpthlp','wpthlp','wpthlp2','wpthlp_ac','wpthlp_bp','wpthlp_bt','wpthlp_cl','wpthlp_dp1','wpthlp_entermfl',\n",
    "                          'wpthlp_exit_mfl','wpthlp_forcing','wpthlp_ma','wpthlp_mc','wpthlp_mfl','wpthlp_mfl_max','wpthlp_mfl_min',\n",
    "                          'wpthlp_pr1','wpthlp_pr2','wpthlp_pr3','wpthlp_sicl','wpthlp_ta','wpthlp_tp','wpthlp_zt']\n",
    "\n",
    "## Other settings to build up case names \n",
    "pertVals = np.asarray(['00'])\n",
    "\n",
    "\n",
    "pertCount = 0\n",
    "for iPert in range(len(pertVals)): \n",
    "    \n",
    "    ## **** IMPORTANT: I've added the 2016 option to the listed files to look at just one year right now\n",
    "        \n",
    "    # listFilesHMG_camh0 = np.sort(glob.glob(dataDir+hmgFileStart+'*pert'+pertVals[iPert]+'*cam.h0*'))\n",
    "    listFilesHTG_camh0 = np.sort(glob.glob(dataDir+htgFileStart+'*pert'+pertVals[iPert]+'*cam.h0*'))\n",
    "    \n",
    "    fileCount=0\n",
    "    for iFile in range(len(listFilesHTG_camh0)):\n",
    "        # with xr.open_dataset(listFilesHMG_camh0[iFile], decode_times=False) as hmgDS: \n",
    "        #     hmgDS         = cesm_correct_time(hmgDS)\n",
    "        #     hmgDS         = hmgDS[budgetTerms]\n",
    "        #     hmgDS['time'] = hmgDS.indexes['time'].to_datetimeindex() \n",
    "        with xr.open_dataset(listFilesHTG_camh0[iFile], decode_times=False) as htgDS: \n",
    "            htgDS         = cesm_correct_time(htgDS)\n",
    "            htgDS         = htgDS[budgetTerms]\n",
    "            htgDS['time'] = htgDS.indexes['time'].to_datetimeindex()\n",
    "        \n",
    "  \n",
    "        iTimeStart_day2  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(2,'D'))))[0]\n",
    "        iTimeStart_day3  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(2,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(3,'D'))))[0]\n",
    "        iTimeStart_day4  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(3,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(4,'D'))))[0]\n",
    "        iTimeStart_day5  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(4,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(5,'D'))))[0]\n",
    "        iTimeStart_day6  = np.where( (htgDS.time.values >= (htgDS.time.values[0] + np.timedelta64(5,'D'))) & \n",
    "                                     (htgDS.time.values <= (htgDS.time.values[0] + np.timedelta64(6,'D'))))[0]\n",
    "        \n",
    "\n",
    "        # Build larger array \n",
    "        if fileCount==0:\n",
    "            # hmg_allDay2      = hmgDS.isel(time=iTimeStart_day2)\n",
    "            # hmg_allDay3      = hmgDS.isel(time=iTimeStart_day3)\n",
    "            # hmg_allDay4      = hmgDS.isel(time=iTimeStart_day4)\n",
    "            # hmg_allDay5      = hmgDS.isel(time=iTimeStart_day5)\n",
    "            # hmg_allDay6      = hmgDS.isel(time=iTimeStart_day6)\n",
    "            \n",
    "            htg_allDay2      = htgDS.isel(time=iTimeStart_day2)\n",
    "            htg_allDay3      = htgDS.isel(time=iTimeStart_day3)\n",
    "            htg_allDay4      = htgDS.isel(time=iTimeStart_day4)\n",
    "            htg_allDay5      = htgDS.isel(time=iTimeStart_day5)\n",
    "            htg_allDay6      = htgDS.isel(time=iTimeStart_day6)\n",
    "            \n",
    "            \n",
    "        else: \n",
    "            # hmg_allDay2      = xr.concat([hmg_allDay2,    hmgDS.isel(time=iTimeStart_day2)],  dim='time')\n",
    "            # hmg_allDay3      = xr.concat([hmg_allDay3,    hmgDS.isel(time=iTimeStart_day3)],  dim='time')\n",
    "            # hmg_allDay4      = xr.concat([hmg_allDay4,    hmgDS.isel(time=iTimeStart_day4)],  dim='time')\n",
    "            # hmg_allDay5      = xr.concat([hmg_allDay5,    hmgDS.isel(time=iTimeStart_day5)],  dim='time')\n",
    "            # hmg_allDay6      = xr.concat([hmg_allDay6,    hmgDS.isel(time=iTimeStart_day6)],  dim='time')\n",
    "            \n",
    "            htg_allDay2      = xr.concat([htg_allDay2,    htgDS.isel(time=iTimeStart_day2)],  dim='time')\n",
    "            htg_allDay3      = xr.concat([htg_allDay3,    htgDS.isel(time=iTimeStart_day3)],  dim='time')\n",
    "            htg_allDay4      = xr.concat([htg_allDay4,    htgDS.isel(time=iTimeStart_day4)],  dim='time')\n",
    "            htg_allDay5      = xr.concat([htg_allDay5,    htgDS.isel(time=iTimeStart_day5)],  dim='time')\n",
    "            htg_allDay6      = xr.concat([htg_allDay6,    htgDS.isel(time=iTimeStart_day6)],  dim='time')   \n",
    "                        \n",
    "        fileCount = fileCount+1\n",
    "        print('Done with day %i of %i ' % (iFile, len(listFilesHTG_camh0)) )\n",
    "            \n",
    "    ## Combine into larger HTG or HMG arrays for all pertlim experiments \n",
    "    # hmg_allDay2  = hmg_allDay2.assign_coords({\"pertlim\": iPert})\n",
    "    # hmg_allDay3  = hmg_allDay3.assign_coords({\"pertlim\": iPert})\n",
    "    # hmg_allDay4  = hmg_allDay4.assign_coords({\"pertlim\": iPert})\n",
    "    # hmg_allDay5  = hmg_allDay5.assign_coords({\"pertlim\": iPert})\n",
    "    # hmg_allDay6  = hmg_allDay6.assign_coords({\"pertlim\": iPert})\n",
    "    \n",
    "    htg_allDay2  = htg_allDay2.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay3  = htg_allDay3.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay4  = htg_allDay4.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay5  = htg_allDay5.assign_coords({\"pertlim\": iPert})\n",
    "    htg_allDay6  = htg_allDay6.assign_coords({\"pertlim\": iPert})\n",
    "\n",
    "  \n",
    "\n",
    "    if pertCount==0: \n",
    "        # hmgDS_pertDay2 = hmg_allDay2\n",
    "        # hmgDS_pertDay3 = hmg_allDay3\n",
    "        # hmgDS_pertDay4 = hmg_allDay4\n",
    "        # hmgDS_pertDay5 = hmg_allDay5\n",
    "        # hmgDS_pertDay6 = hmg_allDay6\n",
    "        \n",
    "        htgDS_pertDay2 = htg_allDay2\n",
    "        htgDS_pertDay3 = htg_allDay3\n",
    "        htgDS_pertDay4 = htg_allDay4\n",
    "        htgDS_pertDay5 = htg_allDay5\n",
    "        htgDS_pertDay6 = htg_allDay6\n",
    "      \n",
    " \n",
    "    else: \n",
    "#         hmgDS_pertDay2 = xr.concat([hmgDS_pertDay2, hmg_allDay2], \"pertlim\")\n",
    "#         hmgDS_pertDay3 = xr.concat([hmgDS_pertDay3, hmg_allDay3], \"pertlim\")\n",
    "#         hmgDS_pertDay4 = xr.concat([hmgDS_pertDay4, hmg_allDay4], \"pertlim\")\n",
    "#         hmgDS_pertDay5 = xr.concat([hmgDS_pertDay5, hmg_allDay5], \"pertlim\")\n",
    "#         hmgDS_pertDay6 = xr.concat([hmgDS_pertDay6, hmg_allDay6], \"pertlim\")\n",
    "        \n",
    "        htgDS_pertDay2 = xr.concat([htgDS_pertDay2, htg_allDay2], \"pertlim\")\n",
    "        htgDS_pertDay3 = xr.concat([htgDS_pertDay3, htg_allDay3], \"pertlim\")\n",
    "        htgDS_pertDay4 = xr.concat([htgDS_pertDay4, htg_allDay4], \"pertlim\")\n",
    "        htgDS_pertDay5 = xr.concat([htgDS_pertDay5, htg_allDay5], \"pertlim\")\n",
    "        htgDS_pertDay6 = xr.concat([htgDS_pertDay6, htg_allDay6], \"pertlim\")\n",
    "    \n",
    "    \n",
    "    print('Done with pertlim ', pertCount)\n",
    "    \n",
    "    pertCount=pertCount+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528c72a1-cf21-4312-86fa-acc962551229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First four times in UTC:\n",
      " ['2018-06-02T05:03:30.000000000' '2018-06-02T05:08:30.000000000'\n",
      " '2018-06-02T05:13:30.000000000' '2018-06-02T05:18:30.000000000'\n",
      " '2018-06-02T05:23:30.000000000']\n",
      "First four times in local:\n",
      " ['2018-06-02T00:03:30.000000000' '2018-06-02T00:08:30.000000000'\n",
      " '2018-06-02T00:13:30.000000000' '2018-06-02T00:18:30.000000000'\n",
      " '2018-06-02T00:23:30.000000000']\n"
     ]
    }
   ],
   "source": [
    "## Convert to local times...\n",
    "# - - - - - - - - - - - - - - \n",
    "# hmgDS_pertDay2_local     = hmgDS_pertDay2.copy(deep=True)\n",
    "# hmgDS_pertDay3_local     = hmgDS_pertDay3.copy(deep=True)\n",
    "# hmgDS_pertDay4_local     = hmgDS_pertDay4.copy(deep=True)\n",
    "# hmgDS_pertDay5_local     = hmgDS_pertDay5.copy(deep=True)\n",
    "# hmgDS_pertDay6_local     = hmgDS_pertDay6.copy(deep=True)\n",
    "\n",
    "htgDS_pertDay2_local     = htgDS_pertDay2.copy(deep=True)\n",
    "htgDS_pertDay3_local     = htgDS_pertDay3.copy(deep=True)\n",
    "htgDS_pertDay4_local     = htgDS_pertDay4.copy(deep=True)\n",
    "htgDS_pertDay5_local     = htgDS_pertDay5.copy(deep=True)\n",
    "htgDS_pertDay6_local     = htgDS_pertDay6.copy(deep=True)\n",
    "\n",
    "\n",
    "\n",
    "# Confirmed that all the times are identical, so using the same local time arrays\n",
    "localTimes_day2 = htgDS_pertDay2['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day3 = htgDS_pertDay3['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day4 = htgDS_pertDay4['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day5 = htgDS_pertDay5['time'].values - np.timedelta64(5,'h')\n",
    "localTimes_day6 = htgDS_pertDay6['time'].values - np.timedelta64(5,'h')\n",
    "\n",
    "\n",
    "# Replace time dimension with local time\n",
    "# hmgDS_pertDay2_local     = hmgDS_pertDay2_local.assign_coords({\"time\": localTimes_day2})\n",
    "# hmgDS_pertDay3_local     = hmgDS_pertDay3_local.assign_coords({\"time\": localTimes_day3})\n",
    "# hmgDS_pertDay4_local     = hmgDS_pertDay4_local.assign_coords({\"time\": localTimes_day4})\n",
    "# hmgDS_pertDay5_local     = hmgDS_pertDay5_local.assign_coords({\"time\": localTimes_day5})\n",
    "# hmgDS_pertDay6_local     = hmgDS_pertDay6_local.assign_coords({\"time\": localTimes_day6})\n",
    "\n",
    "htgDS_pertDay2_local     = htgDS_pertDay2_local.assign_coords({\"time\": localTimes_day2})\n",
    "htgDS_pertDay3_local     = htgDS_pertDay3_local.assign_coords({\"time\": localTimes_day3})\n",
    "htgDS_pertDay4_local     = htgDS_pertDay4_local.assign_coords({\"time\": localTimes_day4})\n",
    "htgDS_pertDay5_local     = htgDS_pertDay5_local.assign_coords({\"time\": localTimes_day5})\n",
    "htgDS_pertDay6_local     = htgDS_pertDay6_local.assign_coords({\"time\": localTimes_day6})\n",
    "\n",
    "print('First four times in UTC:\\n',   htgDS_pertDay2.time.values[0:5])\n",
    "print('First four times in local:\\n', htgDS_pertDay2_local.time.values[0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02549bb-e818-4e2a-ba47-f2646785dd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7517df5-8d75-439a-ad83-9d6e3663d28b",
   "metadata": {},
   "source": [
    "## Process the data as we've done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb59e289-b313-4838-be82-4f2610e30901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_camData(DS):\n",
    " \n",
    "    \n",
    "    ## Interpolate to standard levels \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    print('Beginning interpolation...') \n",
    "    \n",
    "    # Decide on levels to interpoalte to and add to larger arrays\n",
    "    pnew64 = np.arange(200.0,980.0,10.0) \n",
    "\n",
    "    DS = DS.assign_coords({\"levInterp\": pnew64})\n",
    "\n",
    "    varSels = np.asarray(['thlp2','thlp2_bt','thlp2_cl','thlp2_dp1','thlp2_dp2','thlp2_forcing','thlp2_ma','thlp2_mc','thlp2_pd',\n",
    "                         'thlp2_sf','thlp2_ta','thlp2_tp','thlp2_zt', \n",
    "                          'wp2','wp2_ac','wp2_bp','wp2_bt','wp2_cl','wp2_dp1','wp2_dp2','wp2_ma', 'wp2_pd','wp2_pr1','wp2_pr2',\n",
    "                          'wp2_pr3','wp2_sf','wp2_ta','wp2_zt',\n",
    "                          'wp2rcp','wp2rtp','wp2thlp','wp2thvp','wp3_on_wp2','wp3_on_wp2_zt','wprtp',\n",
    "                          'wprtp2','wprtp_ac','wprtp_bp','wprtp_bt','wprtp_cl','wprtp_dp1','wprtp_enter_mfl','wprtp_exit_mfl',\n",
    "                          'wprtp_forcing','wprtp_ma','wprtp_mc','wprtp_mfl','wprtp_mfl_max','wprtp_mfl_min','wprtp_pd',\n",
    "                          'wprtp_pr1','wprtp_pr2','wprtp_pr3','wprtp_sicl','wprtp_ta','wprtp_tp','wprtp_zt',\n",
    "                          'wprtpthlp','wpthlp','wpthlp2','wpthlp_ac','wpthlp_bp','wpthlp_bt','wpthlp_cl','wpthlp_dp1','wpthlp_entermfl',\n",
    "                          'wpthlp_exit_mfl','wpthlp_forcing','wpthlp_ma','wpthlp_mc','wpthlp_mfl','wpthlp_mfl_max','wpthlp_mfl_min',\n",
    "                          'wpthlp_pr1','wpthlp_pr2','wpthlp_pr3','wpthlp_sicl','wpthlp_ta','wpthlp_tp','wpthlp_zt'])\n",
    "\n",
    "    for iVar in range(len(varSels)): \n",
    "        varUnits = DS[varSels[iVar]].units\n",
    "        varName  = DS[varSels[iVar]].long_name\n",
    "\n",
    "        # Interpolate variables and add to larger arrays \n",
    "        interpVar_real = interpolateToPressure_v2(DS, varSels[iVar], pnew64)\n",
    "\n",
    "        DS[varSels[iVar]+'_interp']  = (('time','levInterp','lat','lon'), interpVar_real)\n",
    "\n",
    "        ## Assign attibutes \n",
    "        DS[varSels[iVar]+'_interp'].attrs['units']     = varUnits\n",
    "        DS[varSels[iVar]+'_interp'].attrs['long_name'] = varName\n",
    "\n",
    "    return DS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a5dd66-2d8a-4478-a44d-6f120496364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning interpolation...\n",
      "Beginning interpolation...\n",
      "Beginning interpolation...\n",
      "Beginning interpolation...\n",
      "Beginning interpolation...\n",
      "Done processing HTG DS\n"
     ]
    }
   ],
   "source": [
    "# ## Process data (HMG)\n",
    "# hmgDS_local_processed_day2 = process_camData(hmgDS_pertDay2_local)\n",
    "# hmgDS_local_processed_day3 = process_camData(hmgDS_pertDay3_local)\n",
    "# hmgDS_local_processed_day4 = process_camData(hmgDS_pertDay4_local)\n",
    "# hmgDS_local_processed_day5 = process_camData(hmgDS_pertDay5_local)\n",
    "# hmgDS_local_processed_day6 = process_camData(hmgDS_pertDay6_local)\n",
    "# print('Done processing HMG DS') \n",
    "\n",
    "htgDS_local_processed_day2 = process_camData(htgDS_pertDay2_local)\n",
    "htgDS_local_processed_day3 = process_camData(htgDS_pertDay3_local)\n",
    "htgDS_local_processed_day4 = process_camData(htgDS_pertDay4_local)\n",
    "htgDS_local_processed_day5 = process_camData(htgDS_pertDay5_local)\n",
    "htgDS_local_processed_day6 = process_camData(htgDS_pertDay6_local)\n",
    "print('Done processing HTG DS') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a23c6bed-cd18-4786-a1a6-5483901f711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del hmgDS_pertDay2_local\n",
    "# del hmgDS_pertDay3_local\n",
    "# del hmgDS_pertDay4_local\n",
    "# del hmgDS_pertDay5_local\n",
    "# del hmgDS_pertDay6_local\n",
    "\n",
    "del htgDS_pertDay2_local\n",
    "del htgDS_pertDay3_local\n",
    "del htgDS_pertDay4_local\n",
    "del htgDS_pertDay5_local\n",
    "del htgDS_pertDay6_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff76121-b62e-4450-9ce1-221894b960d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del hmgDS_pertDay2\n",
    "# del hmgDS_pertDay3\n",
    "# del hmgDS_pertDay4\n",
    "# del hmgDS_pertDay5\n",
    "# del hmgDS_pertDay6\n",
    "\n",
    "del htgDS_pertDay2\n",
    "del htgDS_pertDay3\n",
    "del htgDS_pertDay4\n",
    "del htgDS_pertDay5\n",
    "del htgDS_pertDay6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb86c750-f9cf-406a-a9b4-5f0af89fab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed/upsampled HMG files to pickle\n",
      "Saved processed/upsampled HTG files to pickle\n"
     ]
    }
   ],
   "source": [
    "# ## Save upsampled (30m) datasets to pickle files for easy read-in \n",
    "# saveDir = '/glade/work/mdfowler/CLASP/histData/processedData/ens_byLeadDay/'\n",
    "\n",
    "# pickle.dump( hmgDS_local_processed_day2,   open( saveDir+\"realSfc_HMG_day2_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmgDS_local_processed_day3,   open( saveDir+\"realSfc_HMG_day3_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmgDS_local_processed_day4,   open( saveDir+\"realSfc_HMG_day4_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmgDS_local_processed_day5,   open( saveDir+\"realSfc_HMG_day5_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmgDS_local_processed_day6,   open( saveDir+\"realSfc_HMG_day6_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# print('Saved processed/upsampled HMG files to pickle')\n",
    "\n",
    "# pickle.dump( htgDS_local_processed_day2,   open( saveDir+\"realSfc_HTG_day2_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day3,   open( saveDir+\"realSfc_HTG_day3_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day4,   open( saveDir+\"realSfc_HTG_day4_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day5,   open( saveDir+\"realSfc_HTG_day5_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htgDS_local_processed_day6,   open( saveDir+\"realSfc_HTG_day6_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "\n",
    "# # pickle.dump( htg_day2_upsample,   open( saveDir+\"realSfc_HTG_2018day2_30min_pert00.p\", \"wb\" ) )\n",
    "# # pickle.dump( htg_day3_upsample,   open( saveDir+\"realSfc_HTGmult5_2018day3_30min_pert00.p\", \"wb\" ) )\n",
    "# # pickle.dump( htg_day4_upsample,   open( saveDir+\"realSfc_HTGmult5_2018day4_30min_pert00.p\", \"wb\" ) )\n",
    "# # pickle.dump( htg_day5_upsample,   open( saveDir+\"realSfc_HTGmult5_2018day5_30min_pert00.p\", \"wb\" ) )\n",
    "# # pickle.dump( htg_day6_upsample,   open( saveDir+\"realSfc_HTGmult5_2018day6_30min_pert00.p\", \"wb\" ) )\n",
    "# print('Saved processed/upsampled HTG files to pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25959c0f-8f63-4e94-8264-16405ea426c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## took a long time to get through above, so read in the processed data (not resampled)\n",
    "# saveDir = '/glade/work/mdfowler/CLASP/histData/processedData/ens_byLeadDay/'\n",
    "\n",
    "# hmgDS_local_processed_day2 = pickle.load( open( saveDir+\"realSfc_HMG_day2_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# hmgDS_local_processed_day3 = pickle.load( open( saveDir+\"realSfc_HMG_day3_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# hmgDS_local_processed_day4 = pickle.load( open( saveDir+\"realSfc_HMG_day4_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# hmgDS_local_processed_day5 = pickle.load( open( saveDir+\"realSfc_HMG_day5_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# hmgDS_local_processed_day6 = pickle.load( open( saveDir+\"realSfc_HMG_day6_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "\n",
    "# htgDS_local_processed_day2 = pickle.load( open( saveDir+\"realSfc_HTG_day2_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# htgDS_local_processed_day3 = pickle.load( open( saveDir+\"realSfc_HTG_day3_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# htgDS_local_processed_day4 = pickle.load( open( saveDir+\"realSfc_HTG_day4_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# htgDS_local_processed_day5 = pickle.load( open( saveDir+\"realSfc_HTG_day5_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "# htgDS_local_processed_day6 = pickle.load( open( saveDir+\"realSfc_HTG_day6_pert00_CLUBBbudgets.p\" , \"rb\") )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbc257b-5eb6-45c7-a1ee-a58b752fb3a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampled HTG cases (CAM)\n"
     ]
    }
   ],
   "source": [
    "## Upsample to 30m? \n",
    "\n",
    "# hmg_day2_upsample = hmgDS_local_processed_day2.resample(time=\"30Min\").mean()\n",
    "# hmg_day3_upsample = hmgDS_local_processed_day3.resample(time=\"30Min\").mean()\n",
    "# hmg_day4_upsample = hmgDS_local_processed_day4.resample(time=\"30Min\").mean()\n",
    "# hmg_day5_upsample = hmgDS_local_processed_day5.resample(time=\"30Min\").mean()\n",
    "# hmg_day6_upsample = hmgDS_local_processed_day6.resample(time=\"30Min\").mean()\n",
    "# print('Upsampled HMG cases (CAM)')\n",
    "\n",
    "htg_day2_upsample = htgDS_local_processed_day2.resample(time=\"30Min\").mean()\n",
    "htg_day3_upsample = htgDS_local_processed_day3.resample(time=\"30Min\").mean()\n",
    "htg_day4_upsample = htgDS_local_processed_day4.resample(time=\"30Min\").mean()\n",
    "htg_day5_upsample = htgDS_local_processed_day5.resample(time=\"30Min\").mean()\n",
    "htg_day6_upsample = htgDS_local_processed_day6.resample(time=\"30Min\").mean()\n",
    "print('Upsampled HTG cases (CAM)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444b3903-4b64-40c3-bfd8-f37988b3c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed/upsampled HTG files to pickle\n"
     ]
    }
   ],
   "source": [
    "## Save upsampled (30m) datasets to pickle files for easy read-in \n",
    "saveDir = '/glade/work/mdfowler/CLASP/histData/processedData/ens_byLeadDay/'\n",
    "\n",
    "# pickle.dump( hmg_day2_upsample,   open( saveDir+\"realSfc_HMG_day2_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day3_upsample,   open( saveDir+\"realSfc_HMG_day3_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day4_upsample,   open( saveDir+\"realSfc_HMG_day4_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day5_upsample,   open( saveDir+\"realSfc_HMG_day5_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( hmg_day6_upsample,   open( saveDir+\"realSfc_HMG_day6_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# print('Saved processed/upsampled HMG files to pickle')\n",
    "\n",
    "# pickle.dump( htg_day2_upsample,   open( saveDir+\"realSfc_HTG_day2_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htg_day3_upsample,   open( saveDir+\"realSfc_HTG_day3_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htg_day4_upsample,   open( saveDir+\"realSfc_HTG_day4_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htg_day5_upsample,   open( saveDir+\"realSfc_HTG_day5_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "# pickle.dump( htg_day6_upsample,   open( saveDir+\"realSfc_HTG_day6_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "\n",
    "pickle.dump( htg_day2_upsample,   open( saveDir+\"realSfc_HTGmult10_2018day2_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day3_upsample,   open( saveDir+\"realSfc_HTGmult10_2018day3_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day4_upsample,   open( saveDir+\"realSfc_HTGmult10_2018day4_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day5_upsample,   open( saveDir+\"realSfc_HTGmult10_2018day5_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "pickle.dump( htg_day6_upsample,   open( saveDir+\"realSfc_HTGmult10_2018day6_30min_pert00_CLUBBbudgets.p\", \"wb\" ) )\n",
    "print('Saved processed/upsampled HTG files to pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cad111-b17c-4021-8aa3-220dbf382a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5bed5-e3e6-4bfc-b596-70007c2463ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyn_env",
   "language": "python",
   "name": "pyn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
